{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import Preprocessing as preproc\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Links</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Collections</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Timings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>https://www.zomato.com/hyderabad/beyond-flavou...</td>\n",
       "      <td>800</td>\n",
       "      <td>Food Hygiene Rated Restaurants in Hyderabad, C...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paradise</td>\n",
       "      <td>https://www.zomato.com/hyderabad/paradise-gach...</td>\n",
       "      <td>800</td>\n",
       "      <td>Hyderabad's Hottest</td>\n",
       "      <td>Biryani, North Indian, Chinese</td>\n",
       "      <td>11 AM to 11 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flechazo</td>\n",
       "      <td>https://www.zomato.com/hyderabad/flechazo-gach...</td>\n",
       "      <td>1,300</td>\n",
       "      <td>Great Buffets, Hyderabad's Hottest</td>\n",
       "      <td>Asian, Mediterranean, North Indian, Desserts</td>\n",
       "      <td>11:30 AM to 4:30 PM, 6:30 PM to 11 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shah Ghouse Hotel &amp; Restaurant</td>\n",
       "      <td>https://www.zomato.com/hyderabad/shah-ghouse-h...</td>\n",
       "      <td>800</td>\n",
       "      <td>Late Night Restaurants</td>\n",
       "      <td>Biryani, North Indian, Chinese, Seafood, Bever...</td>\n",
       "      <td>12 Noon to 2 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over The Moon Brew Company</td>\n",
       "      <td>https://www.zomato.com/hyderabad/over-the-moon...</td>\n",
       "      <td>1,200</td>\n",
       "      <td>Best Bars &amp; Pubs, Food Hygiene Rated Restauran...</td>\n",
       "      <td>Asian, Continental, North Indian, Chinese, Med...</td>\n",
       "      <td>12noon to 11pm (Mon, Tue, Wed, Thu, Sun), 12no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                 Beyond Flavours   \n",
       "1                        Paradise   \n",
       "2                        Flechazo   \n",
       "3  Shah Ghouse Hotel & Restaurant   \n",
       "4      Over The Moon Brew Company   \n",
       "\n",
       "                                               Links   Cost  \\\n",
       "0  https://www.zomato.com/hyderabad/beyond-flavou...    800   \n",
       "1  https://www.zomato.com/hyderabad/paradise-gach...    800   \n",
       "2  https://www.zomato.com/hyderabad/flechazo-gach...  1,300   \n",
       "3  https://www.zomato.com/hyderabad/shah-ghouse-h...    800   \n",
       "4  https://www.zomato.com/hyderabad/over-the-moon...  1,200   \n",
       "\n",
       "                                         Collections  \\\n",
       "0  Food Hygiene Rated Restaurants in Hyderabad, C...   \n",
       "1                                Hyderabad's Hottest   \n",
       "2                 Great Buffets, Hyderabad's Hottest   \n",
       "3                             Late Night Restaurants   \n",
       "4  Best Bars & Pubs, Food Hygiene Rated Restauran...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  Chinese, Continental, Kebab, European, South I...   \n",
       "1                     Biryani, North Indian, Chinese   \n",
       "2       Asian, Mediterranean, North Indian, Desserts   \n",
       "3  Biryani, North Indian, Chinese, Seafood, Bever...   \n",
       "4  Asian, Continental, North Indian, Chinese, Med...   \n",
       "\n",
       "                                             Timings  \n",
       "0      12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)  \n",
       "1                                     11 AM to 11 PM  \n",
       "2              11:30 AM to 4:30 PM, 6:30 PM to 11 PM  \n",
       "3                                    12 Noon to 2 AM  \n",
       "4  12noon to 11pm (Mon, Tue, Wed, Thu, Sun), 12no...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants_raw = pd.read_csv(r\"data_hyderabad/105_restaurants.csv\")\n",
    "reviews_raw = pd.read_csv(r\"data_hyderabad/10k_reviews.csv\")\n",
    "\n",
    "restaurants_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Angaara Counts 3',\n",
       " 'IndiBlaze',\n",
       " 'Republic Of Noodles - Lemon Tree Hotel',\n",
       " 'Sweet Basket',\n",
       " 'Wich Please'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_raw['Restaurant'].unique())\n",
    "missing_restaurants = set(restaurants_raw['Name']) - set(reviews_raw['Restaurant'])\n",
    "missing_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurant    0\n",
       "Reviewer      0\n",
       "Review        0\n",
       "Rating        0\n",
       "Metadata      0\n",
       "Time          0\n",
       "Pictures      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data = reviews_raw[reviews_raw[\"Rating\"].notna() & reviews_raw[\"Review\"].notna()]\n",
    "reviews_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split UPPERCASE WORDS \n",
    "def splitting_words_process(word):\n",
    "    # only upper case letters\n",
    "    if word.isupper():\n",
    "        return word\n",
    "    \n",
    "    # more than one upper case letter inside\n",
    "    elif re.search(r'[A-Z][a-z]*[A-Z]', word):\n",
    "        split_word = re.findall(r'[A-Z][a-z]*', word)\n",
    "        return ' '.join(split_word)\n",
    "    \n",
    "    # <2 upper case letters\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(lambda x: ' '.join([splitting_words_process(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace 'gud', 'goo', 'gd' with the appropriate 'good'\n",
    "def replace_gud_with_good(text):\n",
    "    if isinstance(text, str):\n",
    "        # Define the regex pattern to match 'gud', 'goo', 'gd' in various capitalizations\n",
    "        pattern = re.compile(r'\\b([Gg][Uu][Dd]|[Gg][Oo][Oo]|[Gg][Dd])\\b')\n",
    "\n",
    "        # Replacement function to check the case of the first letter\n",
    "        def replacement(match):\n",
    "            word = match.group()\n",
    "            # Check if the first letter is uppercase, then return 'Good', else 'good'\n",
    "            if word[0].isupper():\n",
    "                return 'Good'\n",
    "            else:\n",
    "                return 'good'\n",
    "        \n",
    "        # Use re.sub to apply the replacement function\n",
    "        return pattern.sub(replacement, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Review' column to replace the variants of 'good'\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(replace_gud_with_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace 'kk', 'Oke', 'k', 'Ok' with 'ok'\n",
    "def replace_to_ok(text):\n",
    "    if isinstance(text, str):\n",
    "        # Define the regex pattern to match the variants of 'ok'\n",
    "        pattern = re.compile(r'\\b(k|kk|Ok|Oke)\\b', re.IGNORECASE)\n",
    "\n",
    "        # Replacement function to return 'ok' for all matched words\n",
    "        def replacement(match):\n",
    "            return 'ok'\n",
    "        \n",
    "        # Use re.sub to apply the replacement function\n",
    "        return pattern.sub(replacement, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Review' column to replace the variants of 'ok'\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(replace_to_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add space after ! | \" | # | $ | % | & | ( | ) | * | + | , | . | : | ; followed immediately by a word\n",
    "def add_space_after_punctuation(df):\n",
    "\n",
    "    df['Review'] = df['Review'].apply(lambda text: re.sub(r'([\\u0021-\\u0026\\u0028-\\u002C\\u002E\\u003A-\\u003F]+(?=\\w))', r'\\1 ', text) if isinstance(text, str) else text)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "reviews_data = add_space_after_punctuation(reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove gibberish words like \"ggggggggggd\", \"eshjdgue\"\n",
    "def remove_gibberish(text):\n",
    "    cleaned_text = re.sub(r'\\b\\w{15,}\\b', '', text)  # removes 15+ words\n",
    "    cleaned_text = re.sub(r'\\b\\w*(\\w)\\1{2,}\\w*\\b', '', cleaned_text)  # removes words that contain 3+ repeating letters\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(remove_gibberish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace numbers with blank string\n",
    "reviews_data['Review'] = reviews_data['Review'].replace(r'\\d+(\\.\\d+)?', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = reviews_data['Review'].apply(lambda x: preproc.main_pipeline(\n",
    "    x, \n",
    "    print_output=False, \n",
    "    no_stopwords=False,\n",
    "    custom_stopwords=[],\n",
    "    convert_diacritics=True, \n",
    "    no_punctuation=False,\n",
    "    remove_contractions = True,\n",
    "    lowercase=False,\n",
    "    lemmatized=False,\n",
    "    list_pos=[\"n\",\"v\",\"a\",\"r\",\"s\"],\n",
    "    stemmed=False, \n",
    "    pos_tags_list='pos_tuples',\n",
    "    tokenized_output=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    punctuation_pattern = \"[\\u0021-\\u0026\\u0028-\\u002C\\u002E-\\u002F\\u003A-\\u003F\\u005B-\\u005F\\u2010-\\u2028\\ufeff`]+\"\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    return [(token.lower(), pos) for token, pos in tokens if token.lower() not in stopwords and not re.match(punctuation_pattern, token)]\n",
    "\n",
    "reviews_data['Review_Preprocessed'] = preproc.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Reviewer</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pictures</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Name</th>\n",
       "      <th>Links</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Collections</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Timings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Rusha Chakraborty</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "      <td>1 Review , 2 Followers</td>\n",
       "      <td>5/25/2019 15:54</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>https://www.zomato.com/hyderabad/beyond-flavou...</td>\n",
       "      <td>800</td>\n",
       "      <td>Food Hygiene Rated Restaurants in Hyderabad, C...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant           Reviewer  \\\n",
       "0  Beyond Flavours  Rusha Chakraborty   \n",
       "\n",
       "                                              Review Rating  \\\n",
       "0  The ambience was good, food was quite good . h...      5   \n",
       "\n",
       "                 Metadata             Time  Pictures  \\\n",
       "0  1 Review , 2 Followers  5/25/2019 15:54         0   \n",
       "\n",
       "                                 Review_Preprocessed             Name  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...  Beyond Flavours   \n",
       "\n",
       "                                               Links Cost  \\\n",
       "0  https://www.zomato.com/hyderabad/beyond-flavou...  800   \n",
       "\n",
       "                                         Collections  \\\n",
       "0  Food Hygiene Rated Restaurants in Hyderabad, C...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  Chinese, Continental, Kebab, European, South I...   \n",
       "\n",
       "                                         Timings  \n",
       "0  12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = pd.merge(reviews_data, restaurants_raw, left_on='Restaurant',right_on='Name', how='left')\n",
    "data_joined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_joined['Restaurant'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines  \n",
       "0  Chinese, Continental, Kebab, European, South I...  \n",
       "1  Chinese, Continental, Kebab, European, South I...  \n",
       "2  Chinese, Continental, Kebab, European, South I...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = data_joined[['Restaurant', 'Review', 'Review_Preprocessed', 'Cuisines']]\n",
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "model_roberta = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model=model_roberta, tokenizer=tokenizer_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m ner_entity_results \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_joined\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:192\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[0;32m    190\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:114\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:115\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:218\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(model_inputs\u001b[38;5;241m.\u001b[39mdata)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m    226\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1403\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1403\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1417\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    837\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    839\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    840\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    841\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    858\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    859\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    513\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    444\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    445\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 447\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:460\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    459\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 460\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:371\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 371\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    373\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ner_entity_results = pipe(list(data_joined['Review']), aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entities_to_list(df: pd.DataFrame, entities: list[list[dict]]) -> pd.DataFrame:\n",
    "    def extract_entities(text, entity_list):\n",
    "        ents = []\n",
    "        for ent in entity_list:\n",
    "            e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "            if ents and (-1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1) and (ents[-1][\"label\"] == e[\"label\"]):\n",
    "                ents[-1][\"end\"] = e[\"end\"]\n",
    "                continue\n",
    "            ents.append(e)\n",
    "        return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "    df['meals'] = [extract_entities(text, entity_list) for text, entity_list in zip(df['Review'], entities)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>[(soumen, NNP), (das, NNS), (arun, NNP), (grea...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Kodi drumsticks, basket mutton biryani]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Was there for office lunch outing. Rating woul...</td>\n",
       "      <td>[(office, NN), (lunch, NN), (outing, VBG), (ra...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>I really enjoyed the follows.... The entrance,...</td>\n",
       "      <td>[(really, RB), (enjoyed, VBD), (entrance, NN),...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>I came first time in this restaurant. The entr...</td>\n",
       "      <td>[(came, VBD), (first, JJ), (time, NN), (restau...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Pathetic and horrible experience Ambience and ...</td>\n",
       "      <td>[(pathetic, JJ), (horrible, JJ), (experience, ...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ahmed was serving us. Polite and very cooperat...</td>\n",
       "      <td>[(ahmed, NNP), (serving, VBG), (us, PRP), (pol...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Veg gilaffi Kabab]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Restaurant                                             Review  \\\n",
       "0   Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1   Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2   Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "3   Beyond Flavours  Soumen das and Arun was a great guy. Only beca...   \n",
       "4   Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "..              ...                                                ...   \n",
       "95  Beyond Flavours  Was there for office lunch outing. Rating woul...   \n",
       "96  Beyond Flavours  I really enjoyed the follows.... The entrance,...   \n",
       "97  Beyond Flavours  I came first time in this restaurant. The entr...   \n",
       "98  Beyond Flavours  Pathetic and horrible experience Ambience and ...   \n",
       "99  Beyond Flavours  Ahmed was serving us. Polite and very cooperat...   \n",
       "\n",
       "                                  Review_Preprocessed  \\\n",
       "0   [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1   [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2   [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "3   [(soumen, NNP), (das, NNS), (arun, NNP), (grea...   \n",
       "4   [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "..                                                ...   \n",
       "95  [(office, NN), (lunch, NN), (outing, VBG), (ra...   \n",
       "96  [(really, RB), (enjoyed, VBD), (entrance, NN),...   \n",
       "97  [(came, VBD), (first, JJ), (time, NN), (restau...   \n",
       "98  [(pathetic, JJ), (horrible, JJ), (experience, ...   \n",
       "99  [(ahmed, NNP), (serving, VBG), (us, PRP), (pol...   \n",
       "\n",
       "                                             Cuisines  \\\n",
       "0   Chinese, Continental, Kebab, European, South I...   \n",
       "1   Chinese, Continental, Kebab, European, South I...   \n",
       "2   Chinese, Continental, Kebab, European, South I...   \n",
       "3   Chinese, Continental, Kebab, European, South I...   \n",
       "4   Chinese, Continental, Kebab, European, South I...   \n",
       "..                                                ...   \n",
       "95  Chinese, Continental, Kebab, European, South I...   \n",
       "96  Chinese, Continental, Kebab, European, South I...   \n",
       "97  Chinese, Continental, Kebab, European, South I...   \n",
       "98  Chinese, Continental, Kebab, European, South I...   \n",
       "99  Chinese, Continental, Kebab, European, South I...   \n",
       "\n",
       "                                       meals  \n",
       "0                                         []  \n",
       "1                                         []  \n",
       "2                      [Penne Alfredo Pasta]  \n",
       "3                                         []  \n",
       "4   [Kodi drumsticks, basket mutton biryani]  \n",
       "..                                       ...  \n",
       "95                                        []  \n",
       "96                                        []  \n",
       "97                                        []  \n",
       "98                                        []  \n",
       "99                       [Veg gilaffi Kabab]  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = convert_entities_to_list(data_joined, ner_entity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out all the possible labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_list_into_onehot_labels(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Splits a list of comma-separated values in a specified column of a DataFrame into one-hot encoded labels.\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column_name (str): The name of the column containing comma-separated values to be one-hot encoded.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the original column split into a single column containing one-hot encoded vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    dataframe[column_name] = dataframe[column_name].apply(lambda x: x.split(\", \"))\n",
    "\n",
    "    cuisine_encoded = mlb.fit_transform(dataframe[column_name])\n",
    "\n",
    "    dataframe['Cuisine_Vector'] = list(cuisine_encoded)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[('ambience', 'NN'), ('good', 'JJ'), ('food', ...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[('ambience', 'NN'), ('good', 'JJ'), ('pleasan...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[('must', 'MD'), ('try', 'VB'), ('great', 'JJ'...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>['Penne Alfredo Pasta']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [('ambience', 'NN'), ('good', 'JJ'), ('food', ...   \n",
       "1  [('ambience', 'NN'), ('good', 'JJ'), ('pleasan...   \n",
       "2  [('must', 'MD'), ('try', 'VB'), ('great', 'JJ'...   \n",
       "\n",
       "                                            Cuisines                    meals  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...                       []   \n",
       "1  [Chinese, Continental, Kebab, European, South ...                       []   \n",
       "2  [Chinese, Continental, Kebab, European, South ...  ['Penne Alfredo Pasta']   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_joined = pd.read_csv(r\"data_hyderabad/data_preprocessed_classification.csv\")\n",
    "\n",
    "#data_joined['meals'] = data_joined['meals'].apply(lambda x: [meal.lower() for meal in eval(x)])\n",
    "\n",
    "\n",
    "data_joined = split_list_into_onehot_labels(data_joined, 'Cuisines')\n",
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>lasagna, veg Platter, lasagna rolls, beers, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>lassi, Chole bhature, Lassi, chole bhature pan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>Polite, Pan ice cream, pan ice cream, pan ice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>ripan, politley sarvice, fish, pankaj, cake, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>ler, lamb, lamb, Noodles, rice, noodle, chilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>noodles, Sanghai Fried Rice, Fish, sauce, nood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>mutton Haleem, Chicken Fahm Mandi, chicken hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>thukpa, spice, dimsums, chicken Gyoza, dimsums...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>chalega, Pork, beef, meat, meat, veg momo, veg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>gobi methi paratha, paratha, rice, thali, thal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Restaurant  \\\n",
       "0                       10 Downing Street   \n",
       "1                                13 Dhaba   \n",
       "2          3B's - Buddies, Bar & Barbecue   \n",
       "3               AB's - Absolute Barbecues   \n",
       "4                       Absolute Sizzlers   \n",
       "..                                    ...   \n",
       "95             Urban Asia - Kitchen & Bar   \n",
       "96  Yum Yum Tree - The Arabian Food Court   \n",
       "97        Zega - Sheraton Hyderabad Hotel   \n",
       "98               Zing's Northeast Kitchen   \n",
       "99                                eat.fit   \n",
       "\n",
       "                                               Review  \\\n",
       "0   I've been to this place about two times and i ...   \n",
       "1   I didn't go and eat at the Dhaba. I had ordere...   \n",
       "2   Gobind Passionate in serving Polite in nature ...   \n",
       "3   Excellent service by nandan and rahmat and rip...   \n",
       "4   Service was pathetic. Ordered a sizzler with l...   \n",
       "..                                                ...   \n",
       "95  This place is highly recommended. It is workin...   \n",
       "96  It is at th floor of Act Boutique building tha...   \n",
       "97  My husband and I, visited Zega for their dimsu...   \n",
       "98  After so many of goody goody excellent reviews...   \n",
       "99  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "0   've, place, two, times, really, liked, ambienc...   \n",
       "1   go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "2   gobind, passionate, serving, polite, nature, s...   \n",
       "3   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "4   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "..                                                ...   \n",
       "95  place, highly, recommended, working, eat, indi...   \n",
       "96  th, floor, act, boutique, building, entrance, ...   \n",
       "97  husband, visited, zega, dimsum, festival, disa...   \n",
       "98  many, goody, goody, excellent, reviews, n, exc...   \n",
       "99  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                              Review_Preprocessed_Pos  \\\n",
       "0   ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "1   (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "2   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "3   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "4   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "..                                                ...   \n",
       "95  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "96  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "97  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "98  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "99  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                meals  \n",
       "0   lasagna, veg Platter, lasagna rolls, beers, ve...  \n",
       "1   lassi, Chole bhature, Lassi, chole bhature pan...  \n",
       "2   Polite, Pan ice cream, pan ice cream, pan ice ...  \n",
       "3   ripan, politley sarvice, fish, pankaj, cake, b...  \n",
       "4   ler, lamb, lamb, Noodles, rice, noodle, chilli...  \n",
       "..                                                ...  \n",
       "95  noodles, Sanghai Fried Rice, Fish, sauce, nood...  \n",
       "96  mutton Haleem, Chicken Fahm Mandi, chicken hal...  \n",
       "97  thukpa, spice, dimsums, chicken Gyoza, dimsums...  \n",
       "98  chalega, Pork, beef, meat, meat, veg momo, veg...  \n",
       "99  gobi methi paratha, paratha, rice, thali, thal...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows where 'meals' column is empty\n",
    "data_joined = data_joined[data_joined['meals'].apply(lambda x: x != \"[]\")]\n",
    "\n",
    "# Convert 'Review_Preprocessed' from string to list of tuples\n",
    "data_joined['Review_Preprocessed'] = data_joined['Review_Preprocessed'].apply(eval)\n",
    "\n",
    "# Extract tokens from 'Review_Preprocessed' and store in a new column\n",
    "data_joined['Review_Preprocessed_No_Pos'] = data_joined['Review_Preprocessed'].apply(lambda x: [token for token, pos in x])\n",
    "\n",
    "# Reorder columns to place 'Review_Preprocessed_No_Pos' after 'Review_Preprocessed'\n",
    "cols = list(data_joined.columns)\n",
    "review_index = cols.index('Review_Preprocessed')\n",
    "cols.insert(review_index + 1, cols.pop(cols.index('Review_Preprocessed_No_Pos')))\n",
    "data_joined = data_joined[cols]\n",
    "\n",
    "# Create a copy of the dataframe for further processing\n",
    "data_preprocessed_many_rows = data_joined.copy()\n",
    "\n",
    "# Group by 'Restaurant' and aggregate reviews and meals\n",
    "data_preprocessed_groupedby_restaurant = data_preprocessed_many_rows.groupby('Restaurant').agg({\n",
    "    'Review': lambda x: ', '.join(x),\n",
    "    'Review_Preprocessed_No_Pos': lambda x: ', '.join([', '.join(tokens) for tokens in x]),\n",
    "    'Review_Preprocessed': lambda x: ', '.join([', '.join([f\"({token}, {pos})\" for token, pos in tokens]) for tokens in x]),\n",
    "    'meals': lambda x: ', '.join([meal for sublist in x for meal in eval(sublist)])\n",
    "}).reset_index().rename(columns={\"Review_Preprocessed\": \"Review_Preprocessed_Pos\"})\n",
    "\n",
    "data_preprocessed_groupedby_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>lasagna, veg Platter, lasagna rolls, beers, ve...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>lassi, Chole bhature, Lassi, chole bhature pan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>Polite, Pan ice cream, pan ice cream, pan ice ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>ripan, politley sarvice, fish, pankaj, cake, b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>ler, lamb, lamb, Noodles, rice, noodle, chilli...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>noodles, Sanghai Fried Rice, Fish, sauce, nood...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>mutton Haleem, Chicken Fahm Mandi, chicken hal...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>thukpa, spice, dimsums, chicken Gyoza, dimsums...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>chalega, Pork, beef, meat, meat, veg momo, veg...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>gobi methi paratha, paratha, rice, thali, thal...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Restaurant  \\\n",
       "0                       10 Downing Street   \n",
       "1                                13 Dhaba   \n",
       "2          3B's - Buddies, Bar & Barbecue   \n",
       "3               AB's - Absolute Barbecues   \n",
       "4                       Absolute Sizzlers   \n",
       "..                                    ...   \n",
       "95             Urban Asia - Kitchen & Bar   \n",
       "96  Yum Yum Tree - The Arabian Food Court   \n",
       "97        Zega - Sheraton Hyderabad Hotel   \n",
       "98               Zing's Northeast Kitchen   \n",
       "99                                eat.fit   \n",
       "\n",
       "                                               Review  \\\n",
       "0   I've been to this place about two times and i ...   \n",
       "1   I didn't go and eat at the Dhaba. I had ordere...   \n",
       "2   Gobind Passionate in serving Polite in nature ...   \n",
       "3   Excellent service by nandan and rahmat and rip...   \n",
       "4   Service was pathetic. Ordered a sizzler with l...   \n",
       "..                                                ...   \n",
       "95  This place is highly recommended. It is workin...   \n",
       "96  It is at th floor of Act Boutique building tha...   \n",
       "97  My husband and I, visited Zega for their dimsu...   \n",
       "98  After so many of goody goody excellent reviews...   \n",
       "99  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "0   've, place, two, times, really, liked, ambienc...   \n",
       "1   go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "2   gobind, passionate, serving, polite, nature, s...   \n",
       "3   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "4   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "..                                                ...   \n",
       "95  place, highly, recommended, working, eat, indi...   \n",
       "96  th, floor, act, boutique, building, entrance, ...   \n",
       "97  husband, visited, zega, dimsum, festival, disa...   \n",
       "98  many, goody, goody, excellent, reviews, n, exc...   \n",
       "99  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                              Review_Preprocessed_Pos  \\\n",
       "0   ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "1   (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "2   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "3   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "4   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "..                                                ...   \n",
       "95  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "96  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "97  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "98  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "99  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                meals  \\\n",
       "0   lasagna, veg Platter, lasagna rolls, beers, ve...   \n",
       "1   lassi, Chole bhature, Lassi, chole bhature pan...   \n",
       "2   Polite, Pan ice cream, pan ice cream, pan ice ...   \n",
       "3   ripan, politley sarvice, fish, pankaj, cake, b...   \n",
       "4   ler, lamb, lamb, Noodles, rice, noodle, chilli...   \n",
       "..                                                ...   \n",
       "95  noodles, Sanghai Fried Rice, Fish, sauce, nood...   \n",
       "96  mutton Haleem, Chicken Fahm Mandi, chicken hal...   \n",
       "97  thukpa, spice, dimsums, chicken Gyoza, dimsums...   \n",
       "98  chalega, Pork, beef, meat, meat, veg momo, veg...   \n",
       "99  gobi methi paratha, paratha, rice, thali, thal...   \n",
       "\n",
       "                                       Cuisine_Vector  \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "4   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "..                                                ...  \n",
       "95  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "96  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "97  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "98  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "99  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge 'Cuisine_Vector' from 'data_joined' into 'data_preprocessed_groupedby_restaurant'\n",
    "data_preprocessed_groupedby_restaurant = data_preprocessed_groupedby_restaurant.merge(\n",
    "    data_joined[['Restaurant', 'Cuisine_Vector']],\n",
    "    on='Restaurant',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data_preprocessed_groupedby_restaurant.drop_duplicates(subset=['Restaurant'], inplace=True)\n",
    "data_preprocessed_groupedby_restaurant.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_groupedby_restaurant['meals'] = data_preprocessed_groupedby_restaurant['meals'].apply(lambda x: x.split())\n",
    "data_preproc_grouped = data_preprocessed_groupedby_restaurant.copy()\n",
    "\n",
    "data_joined['meals'] = data_joined['meals'].apply(lambda x: [meal.lower() for meal in eval(x)])\n",
    "data_preproc_review_per_row = data_joined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>[lasagna,, veg, Platter,, lasagna, rolls,, bee...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>[lassi,, Chole, bhature,, Lassi,, chole, bhatu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>[Polite,, Pan, ice, cream,, pan, ice, cream,, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>[ripan,, politley, sarvice,, fish,, pankaj,, c...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>[ler,, lamb,, lamb,, Noodles,, rice,, noodle,,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>[noodles,, Sanghai, Fried, Rice,, Fish,, sauce...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>[mutton, Haleem,, Chicken, Fahm, Mandi,, chick...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>[thukpa,, spice,, dimsums,, chicken, Gyoza,, d...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>[chalega,, Pork,, beef,, meat,, meat,, veg, mo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>[gobi, methi, paratha,, paratha,, rice,, thali...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Restaurant  \\\n",
       "0                         10 Downing Street   \n",
       "61                                 13 Dhaba   \n",
       "132          3B's - Buddies, Bar & Barbecue   \n",
       "159               AB's - Absolute Barbecues   \n",
       "193                       Absolute Sizzlers   \n",
       "...                                     ...   \n",
       "5561             Urban Asia - Kitchen & Bar   \n",
       "5627  Yum Yum Tree - The Arabian Food Court   \n",
       "5700        Zega - Sheraton Hyderabad Hotel   \n",
       "5753               Zing's Northeast Kitchen   \n",
       "5832                                eat.fit   \n",
       "\n",
       "                                                 Review  \\\n",
       "0     I've been to this place about two times and i ...   \n",
       "61    I didn't go and eat at the Dhaba. I had ordere...   \n",
       "132   Gobind Passionate in serving Polite in nature ...   \n",
       "159   Excellent service by nandan and rahmat and rip...   \n",
       "193   Service was pathetic. Ordered a sizzler with l...   \n",
       "...                                                 ...   \n",
       "5561  This place is highly recommended. It is workin...   \n",
       "5627  It is at th floor of Act Boutique building tha...   \n",
       "5700  My husband and I, visited Zega for their dimsu...   \n",
       "5753  After so many of goody goody excellent reviews...   \n",
       "5832  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                             Review_Preprocessed_No_Pos  \\\n",
       "0     've, place, two, times, really, liked, ambienc...   \n",
       "61    go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "132   gobind, passionate, serving, polite, nature, s...   \n",
       "159   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "193   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "...                                                 ...   \n",
       "5561  place, highly, recommended, working, eat, indi...   \n",
       "5627  th, floor, act, boutique, building, entrance, ...   \n",
       "5700  husband, visited, zega, dimsum, festival, disa...   \n",
       "5753  many, goody, goody, excellent, reviews, n, exc...   \n",
       "5832  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                                Review_Preprocessed_Pos  \\\n",
       "0     ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "61    (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "132   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "159   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "193   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "...                                                 ...   \n",
       "5561  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "5627  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "5700  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "5753  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "5832  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                  meals  \\\n",
       "0     [lasagna,, veg, Platter,, lasagna, rolls,, bee...   \n",
       "61    [lassi,, Chole, bhature,, Lassi,, chole, bhatu...   \n",
       "132   [Polite,, Pan, ice, cream,, pan, ice, cream,, ...   \n",
       "159   [ripan,, politley, sarvice,, fish,, pankaj,, c...   \n",
       "193   [ler,, lamb,, lamb,, Noodles,, rice,, noodle,,...   \n",
       "...                                                 ...   \n",
       "5561  [noodles,, Sanghai, Fried, Rice,, Fish,, sauce...   \n",
       "5627  [mutton, Haleem,, Chicken, Fahm, Mandi,, chick...   \n",
       "5700  [thukpa,, spice,, dimsums,, chicken, Gyoza,, d...   \n",
       "5753  [chalega,, Pork,, beef,, meat,, meat,, veg, mo...   \n",
       "5832  [gobi, methi, paratha,, paratha,, rice,, thali...   \n",
       "\n",
       "                                         Cuisine_Vector  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "61    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "132   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "159   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "193   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "5561  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "5627  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5700  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5753  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "5832  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproc_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne alfredo pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[food, good, ordered, kodi, drumsticks, basket...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi drumsticks, basket mutton biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[well, reading, many, reviews, finally, visite...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa fish, basket biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Came for the birthday treat of a close friend....</td>\n",
       "      <td>[(came, NN), (birthday, JJ), (treat, NN), (clo...</td>\n",
       "      <td>[came, birthday, treat, close, friend, perfect...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[chili honey lotus stem]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food was very good. Soup was as expected. In s...</td>\n",
       "      <td>[(food, NN), (good, JJ), (soup, NNP), (expecte...</td>\n",
       "      <td>[food, good, soup, expected, starters, ordered...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[soup, honey chilli lotus]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9949</th>\n",
       "      <td>Chinese Pavilion</td>\n",
       "      <td>Chinese Pavilion in Banjara H Ills was the fir...</td>\n",
       "      <td>[(chinese, JJ), (pavilion, NNP), (banjara, NNP...</td>\n",
       "      <td>[chinese, pavilion, banjara, h, ills, first, c...</td>\n",
       "      <td>[Chinese, Seafood]</td>\n",
       "      <td>[noodles, fried rice, lamb, brownie, rice, lamb]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>Chinese Pavilion</td>\n",
       "      <td>Madhumathi Mahajan Well to start with nice cou...</td>\n",
       "      <td>[(madhumathi, NNP), (mahajan, NNP), (well, NNP...</td>\n",
       "      <td>[madhumathi, mahajan, well, start, nice, court...</td>\n",
       "      <td>[Chinese, Seafood]</td>\n",
       "      <td>[jade chicken soup, spinach, house salad, cabb...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9951</th>\n",
       "      <td>Chinese Pavilion</td>\n",
       "      <td>This place has never disappointed us.. The foo...</td>\n",
       "      <td>[(place, NN), (never, RB), (disappointed, VBN)...</td>\n",
       "      <td>[place, never, disappointed, us, food, courteo...</td>\n",
       "      <td>[Chinese, Seafood]</td>\n",
       "      <td>[rice, oil, fish, garlic noodles, chicken]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9952</th>\n",
       "      <td>Chinese Pavilion</td>\n",
       "      <td>Bad rating is mainly because of \" Chicken Bone...</td>\n",
       "      <td>[(bad, NNP), (rating, NN), (mainly, RB), (chic...</td>\n",
       "      <td>[bad, rating, mainly, chicken, bone, found, ve...</td>\n",
       "      <td>[Chinese, Seafood]</td>\n",
       "      <td>[chicken bone, hot and sour soup, sprouts, kim...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Chinese Pavilion</td>\n",
       "      <td>Checked in here to try some delicious chinese ...</td>\n",
       "      <td>[(checked, VBN), (try, VB), (delicious, JJ), (...</td>\n",
       "      <td>[checked, try, delicious, chinese, food, seen,...</td>\n",
       "      <td>[Chinese, Seafood]</td>\n",
       "      <td>[manchow soup, pepper chicken starter, kimchi,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5904 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Restaurant                                             Review  \\\n",
       "2      Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "4      Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "7      Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "9      Beyond Flavours  Came for the birthday treat of a close friend....   \n",
       "12     Beyond Flavours  Food was very good. Soup was as expected. In s...   \n",
       "...                ...                                                ...   \n",
       "9949  Chinese Pavilion  Chinese Pavilion in Banjara H Ills was the fir...   \n",
       "9950  Chinese Pavilion  Madhumathi Mahajan Well to start with nice cou...   \n",
       "9951  Chinese Pavilion  This place has never disappointed us.. The foo...   \n",
       "9952  Chinese Pavilion  Bad rating is mainly because of \" Chicken Bone...   \n",
       "9954  Chinese Pavilion  Checked in here to try some delicious chinese ...   \n",
       "\n",
       "                                    Review_Preprocessed  \\\n",
       "2     [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "4     [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "7     [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "9     [(came, NN), (birthday, JJ), (treat, NN), (clo...   \n",
       "12    [(food, NN), (good, JJ), (soup, NNP), (expecte...   \n",
       "...                                                 ...   \n",
       "9949  [(chinese, JJ), (pavilion, NNP), (banjara, NNP...   \n",
       "9950  [(madhumathi, NNP), (mahajan, NNP), (well, NNP...   \n",
       "9951  [(place, NN), (never, RB), (disappointed, VBN)...   \n",
       "9952  [(bad, NNP), (rating, NN), (mainly, RB), (chic...   \n",
       "9954  [(checked, VBN), (try, VB), (delicious, JJ), (...   \n",
       "\n",
       "                             Review_Preprocessed_No_Pos  \\\n",
       "2     [must, try, great, food, great, ambience, thnx...   \n",
       "4     [food, good, ordered, kodi, drumsticks, basket...   \n",
       "7     [well, reading, many, reviews, finally, visite...   \n",
       "9     [came, birthday, treat, close, friend, perfect...   \n",
       "12    [food, good, soup, expected, starters, ordered...   \n",
       "...                                                 ...   \n",
       "9949  [chinese, pavilion, banjara, h, ills, first, c...   \n",
       "9950  [madhumathi, mahajan, well, start, nice, court...   \n",
       "9951  [place, never, disappointed, us, food, courteo...   \n",
       "9952  [bad, rating, mainly, chicken, bone, found, ve...   \n",
       "9954  [checked, try, delicious, chinese, food, seen,...   \n",
       "\n",
       "                                               Cuisines  \\\n",
       "2     [Chinese, Continental, Kebab, European, South ...   \n",
       "4     [Chinese, Continental, Kebab, European, South ...   \n",
       "7     [Chinese, Continental, Kebab, European, South ...   \n",
       "9     [Chinese, Continental, Kebab, European, South ...   \n",
       "12    [Chinese, Continental, Kebab, European, South ...   \n",
       "...                                                 ...   \n",
       "9949                                 [Chinese, Seafood]   \n",
       "9950                                 [Chinese, Seafood]   \n",
       "9951                                 [Chinese, Seafood]   \n",
       "9952                                 [Chinese, Seafood]   \n",
       "9954                                 [Chinese, Seafood]   \n",
       "\n",
       "                                                  meals  \\\n",
       "2                                 [penne alfredo pasta]   \n",
       "4              [kodi drumsticks, basket mutton biryani]   \n",
       "7            [corn, tawa fish, basket biryani, biryani]   \n",
       "9                              [chili honey lotus stem]   \n",
       "12                           [soup, honey chilli lotus]   \n",
       "...                                                 ...   \n",
       "9949   [noodles, fried rice, lamb, brownie, rice, lamb]   \n",
       "9950  [jade chicken soup, spinach, house salad, cabb...   \n",
       "9951         [rice, oil, fish, garlic noodles, chicken]   \n",
       "9952  [chicken bone, hot and sour soup, sprouts, kim...   \n",
       "9954  [manchow soup, pepper chicken starter, kimchi,...   \n",
       "\n",
       "                                         Cuisine_Vector  \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "7     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "9     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "12    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "...                                                 ...  \n",
       "9949  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "9950  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "9951  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "9952  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "9954  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "\n",
       "[5904 rows x 7 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproc_review_per_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Futher steps**\n",
    "\n",
    "## Final preprocessing steps:\n",
    "1. Create two datasets: \n",
    "2. 1st with 9k+ rows, so every review has a label vector\n",
    "3. 2nd with 105 rows, so every restaurant has a joint review vector of food names\n",
    "\n",
    "\n",
    "## Modeling\n",
    "I see two possible approaches to solve this problem. \n",
    "\n",
    "1. We treat the dishes as categorical instances and build a model that takes counts\n",
    "It will be a categorical to categorical model so Decision Tree or Naive Bayes should do\n",
    "\n",
    "2. Encoding the words with (no tf-idf not appropriate for this appraoch coz no documents), Word2Vec and Glove and categorizing the labels based on those. Here we can think about adding/multiplying the vectors to get the overall or dot product and using this numerical representation for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Word2Vec_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne alfredo pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.09448282, 0.20344338, 0.1122126, -0.5233302...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[food, good, ordered, kodi, drumsticks, basket...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi drumsticks, basket mutton biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.07203948, 0.27816552, -0.031543814, -0.4987...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[well, reading, many, reviews, finally, visite...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa fish, basket biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.050413016, 0.18470684, 0.061515264, -0.6186...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Came for the birthday treat of a close friend....</td>\n",
       "      <td>[(came, NN), (birthday, JJ), (treat, NN), (clo...</td>\n",
       "      <td>[came, birthday, treat, close, friend, perfect...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[chili honey lotus stem]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.004651741, 0.12694506, 0.08261496, -0.63544...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food was very good. Soup was as expected. In s...</td>\n",
       "      <td>[(food, NN), (good, JJ), (soup, NNP), (expecte...</td>\n",
       "      <td>[food, good, soup, expected, starters, ordered...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[soup, honey chilli lotus]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.11184975, 0.23971973, 0.05759144, -0.590902...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Restaurant                                             Review  \\\n",
       "2   Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "4   Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "7   Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "9   Beyond Flavours  Came for the birthday treat of a close friend....   \n",
       "12  Beyond Flavours  Food was very good. Soup was as expected. In s...   \n",
       "\n",
       "                                  Review_Preprocessed  \\\n",
       "2   [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "4   [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "7   [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "9   [(came, NN), (birthday, JJ), (treat, NN), (clo...   \n",
       "12  [(food, NN), (good, JJ), (soup, NNP), (expecte...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "2   [must, try, great, food, great, ambience, thnx...   \n",
       "4   [food, good, ordered, kodi, drumsticks, basket...   \n",
       "7   [well, reading, many, reviews, finally, visite...   \n",
       "9   [came, birthday, treat, close, friend, perfect...   \n",
       "12  [food, good, soup, expected, starters, ordered...   \n",
       "\n",
       "                                             Cuisines  \\\n",
       "2   [Chinese, Continental, Kebab, European, South ...   \n",
       "4   [Chinese, Continental, Kebab, European, South ...   \n",
       "7   [Chinese, Continental, Kebab, European, South ...   \n",
       "9   [Chinese, Continental, Kebab, European, South ...   \n",
       "12  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                         meals  \\\n",
       "2                        [penne alfredo pasta]   \n",
       "4     [kodi drumsticks, basket mutton biryani]   \n",
       "7   [corn, tawa fish, basket biryani, biryani]   \n",
       "9                     [chili honey lotus stem]   \n",
       "12                  [soup, honey chilli lotus]   \n",
       "\n",
       "                                       Cuisine_Vector  \\\n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                                      Word2Vec_Vector  \n",
       "2   [0.09448282, 0.20344338, 0.1122126, -0.5233302...  \n",
       "4   [0.07203948, 0.27816552, -0.031543814, -0.4987...  \n",
       "7   [0.050413016, 0.18470684, 0.061515264, -0.6186...  \n",
       "9   [0.004651741, 0.12694506, 0.08261496, -0.63544...  \n",
       "12  [0.11184975, 0.23971973, 0.05759144, -0.590902...  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model on the 'meals' column\n",
    "reviews_skipgram_model = Word2Vec(\n",
    "    sentences=data_preproc_review_per_row[\"Review_Preprocessed_No_Pos\"].tolist(),\n",
    "    vector_size=25,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # Skip-gram model\n",
    ")\n",
    "\n",
    "#Generate vectors for each review\n",
    "def get_review_vector(review):\n",
    "    words = review\n",
    "    vector = sum(reviews_skipgram_model.wv[word] for word in words if word in reviews_skipgram_model.wv)\n",
    "    return vector / len(words) if words else [0] * 25\n",
    "\n",
    "# Apply the function to each review and create a new column\n",
    "data_preproc_review_per_row['Word2Vec_Vector'] = data_preproc_review_per_row['Review_Preprocessed_No_Pos'].apply(get_review_vector)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data_preproc_review_per_row.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cooccurrence_matrix_sentence_generator(preproc_sentences):\n",
    "    # Flatten the list of sentences to get all unique words\n",
    "    unique_words = list(set(word for sentence in preproc_sentences for word in sentence))\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    vocab_size = len(unique_words)\n",
    "\n",
    "    # Initialize the co-occurrence matrix\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=int)\n",
    "\n",
    "    # Compute co-occurrences\n",
    "    for sentence in tqdm(preproc_sentences):\n",
    "        word_indices = [word_index[word] for word in sentence if word in word_index]\n",
    "        for i in range(len(word_indices)):\n",
    "            co_matrix[word_indices[i], word_indices[i:]] += 1\n",
    "            co_matrix[word_indices[i:], word_indices[i]] += 1\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "    # Return the co-occurrence matrix\n",
    "    return co_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 21.32it/s]\n"
     ]
    }
   ],
   "source": [
    "co_occr_matrix = cooccurrence_matrix_sentence_generator(data_preproc_grouped['meals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chicken</th>\n",
       "      <th>chicken,</th>\n",
       "      <th>biryani,</th>\n",
       "      <th>Chicken</th>\n",
       "      <th>rice,</th>\n",
       "      <th>veg</th>\n",
       "      <th>soup,</th>\n",
       "      <th>veg,</th>\n",
       "      <th>pizza,</th>\n",
       "      <th>curry,</th>\n",
       "      <th>...</th>\n",
       "      <th>Sm,</th>\n",
       "      <th>nutral,</th>\n",
       "      <th>doubles,</th>\n",
       "      <th>uttappam,</th>\n",
       "      <th>WASTE,</th>\n",
       "      <th>ulavacharu,</th>\n",
       "      <th>Dominos,</th>\n",
       "      <th>Dominos</th>\n",
       "      <th>rayalsima</th>\n",
       "      <th>domin,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>chicken</th>\n",
       "      <td>2326</td>\n",
       "      <td>2011</td>\n",
       "      <td>1556</td>\n",
       "      <td>1438</td>\n",
       "      <td>1305</td>\n",
       "      <td>1075</td>\n",
       "      <td>829</td>\n",
       "      <td>922</td>\n",
       "      <td>543</td>\n",
       "      <td>820</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chicken,</th>\n",
       "      <td>2011</td>\n",
       "      <td>2138</td>\n",
       "      <td>1452</td>\n",
       "      <td>1370</td>\n",
       "      <td>1292</td>\n",
       "      <td>1011</td>\n",
       "      <td>820</td>\n",
       "      <td>879</td>\n",
       "      <td>480</td>\n",
       "      <td>882</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryani,</th>\n",
       "      <td>1556</td>\n",
       "      <td>1452</td>\n",
       "      <td>1852</td>\n",
       "      <td>1108</td>\n",
       "      <td>1104</td>\n",
       "      <td>775</td>\n",
       "      <td>528</td>\n",
       "      <td>648</td>\n",
       "      <td>136</td>\n",
       "      <td>709</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chicken</th>\n",
       "      <td>1438</td>\n",
       "      <td>1370</td>\n",
       "      <td>1108</td>\n",
       "      <td>1190</td>\n",
       "      <td>895</td>\n",
       "      <td>696</td>\n",
       "      <td>630</td>\n",
       "      <td>611</td>\n",
       "      <td>416</td>\n",
       "      <td>552</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rice,</th>\n",
       "      <td>1305</td>\n",
       "      <td>1292</td>\n",
       "      <td>1104</td>\n",
       "      <td>895</td>\n",
       "      <td>1208</td>\n",
       "      <td>668</td>\n",
       "      <td>668</td>\n",
       "      <td>559</td>\n",
       "      <td>252</td>\n",
       "      <td>688</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ulavacharu,</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominos,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominos</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rayalsima</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>domin,</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5995 rows × 5995 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             chicken  chicken,  biryani,  Chicken  rice,   veg  soup,  veg,  \\\n",
       "chicken         2326      2011      1556     1438   1305  1075    829   922   \n",
       "chicken,        2011      2138      1452     1370   1292  1011    820   879   \n",
       "biryani,        1556      1452      1852     1108   1104   775    528   648   \n",
       "Chicken         1438      1370      1108     1190    895   696    630   611   \n",
       "rice,           1305      1292      1104      895   1208   668    668   559   \n",
       "...              ...       ...       ...      ...    ...   ...    ...   ...   \n",
       "ulavacharu,        3         2         7        0      1     2      0     0   \n",
       "Dominos,           0         0         0        0      0     1      0     1   \n",
       "Dominos            0         0         0        0      0     1      0     1   \n",
       "rayalsima          1         1         4        0      1     2      0     0   \n",
       "domin,             0         0         0        0      0     1      0     1   \n",
       "\n",
       "             pizza,  curry,  ...  Sm,  nutral,  doubles,  uttappam,  WASTE,  \\\n",
       "chicken         543     820  ...    1        6         1          0       1   \n",
       "chicken,        480     882  ...    1        3         1          0       1   \n",
       "biryani,        136     709  ...    1       10         0          0       0   \n",
       "Chicken         416     552  ...    0        0         0          0       1   \n",
       "rice,           252     688  ...    0        1         1          0       0   \n",
       "...             ...     ...  ...  ...      ...       ...        ...     ...   \n",
       "ulavacharu,       0       0  ...    0        1         0          0       0   \n",
       "Dominos,         10       0  ...    0        0         0          1       0   \n",
       "Dominos           9       0  ...    0        0         0          1       0   \n",
       "rayalsima         0       0  ...    0        1         0          0       0   \n",
       "domin,            2       0  ...    0        0         0          1       0   \n",
       "\n",
       "             ulavacharu,  Dominos,  Dominos  rayalsima  domin,  \n",
       "chicken                3         0        0          1       0  \n",
       "chicken,               2         0        0          1       0  \n",
       "biryani,               7         0        0          4       0  \n",
       "Chicken                0         0        0          0       0  \n",
       "rice,                  1         0        0          1       0  \n",
       "...                  ...       ...      ...        ...     ...  \n",
       "ulavacharu,            2         0        0          1       0  \n",
       "Dominos,               0         2        1          0       1  \n",
       "Dominos                0         1        2          0       1  \n",
       "rayalsima              1         0        0          2       0  \n",
       "domin,                 0         1        1          0       2  \n",
       "\n",
       "[5995 rows x 5995 columns]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[239], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train GloVe model on the co-occurrence matrix\u001b[39;00m\n\u001b[0;32m      8\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m GloVe(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# 25 is the embedding dimension\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mglove_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco_occr_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate vectors for each review using GloVe embeddings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_review_vector_glove\u001b[39m(review, embeddings, vocab, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\mittens_base.py:239\u001b[0m, in \u001b[0;36mGloVeBase.fit\u001b[1;34m(self, X, fixed_initialization)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, fixed_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124;03m\"\"\"Run GloVe and return the new matrix.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        of X.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGloVeBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_initialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_initialization\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\mittens_base.py:81\u001b[0m, in \u001b[0;36mMittensBase.fit\u001b[1;34m(self, X, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_dimensions(\n\u001b[0;32m     78\u001b[0m     X, vocab, initial_embedding_dict\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m weights, log_coincidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(X)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_coincidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                 \u001b[49m\u001b[43minitial_embedding_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_embedding_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfixed_initialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_initialization\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\np_mittens.py:64\u001b[0m, in \u001b[0;36mMittens._fit\u001b[1;34m(self, coincidence, weights, log_coincidence, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m     63\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_prediction()\n\u001b[1;32m---> 64\u001b[0m     gradients, error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gradients_and_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_coincidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_shapes(gradients)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mappend(error)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\np_mittens.py:121\u001b[0m, in \u001b[0;36mMittens._get_gradients_and_error\u001b[1;34m(self, predictions, log_coincidence, weights)\u001b[0m\n\u001b[0;32m    119\u001b[0m wgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC)\n\u001b[0;32m    120\u001b[0m cgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW)\n\u001b[1;32m--> 121\u001b[0m bwgrad \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_diffs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    122\u001b[0m bcgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    123\u001b[0m error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(weights, diffs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "from mittens import GloVe\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train GloVe model on the co-occurrence matrix\n",
    "glove_model = GloVe(n=25, max_iter=150, display_progress=10)  # 25 is the embedding dimension\n",
    "embeddings = glove_model.fit(co_occr_matrix)\n",
    "\n",
    "# Generate vectors for each review using GloVe embeddings\n",
    "def get_review_vector_glove(review, embeddings, vocab, vector_size=25):\n",
    "    words = review.split()\n",
    "    vector = np.sum([embeddings[vocab[word]] for word in words if word in vocab], axis=0)\n",
    "    return vector / len(words) if words else np.zeros(vector_size)\n",
    "\n",
    "# Apply the function to each review and create a new column\n",
    "data_preproc_review_per_row['Glove_Vector'] = data_preproc_review_per_row['Review_Preprocessed_No_Pos'].apply(lambda x: get_review_vector_glove(x, embeddings, vocab))\n",
    "\n",
    "# Assuming 'cuisine_vector' is the target column\n",
    "X = np.array(data_preproc_review_per_row['Glove_Vector'].tolist())\n",
    "y = np.array(data_preproc_review_per_row['cuisine_vector'].tolist())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the cuisine vector\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_preproc_review_per_row['Word2Vec_Vector'].to_list()\n",
    "y = data_preproc_review_per_row['Cuisine_Vector'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5904, 123326)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "\n",
    "X_one_hot = data_preproc_review_per_row['meals'].copy()\n",
    "X_one_hot = onehot.fit_transform(X).toarray()\n",
    "X_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.32      0.28       113\n",
      "           1       0.07      0.30      0.11        99\n",
      "           2       0.19      0.15      0.17        40\n",
      "           3       0.00      0.00      0.00       240\n",
      "           4       0.00      0.00      0.00        31\n",
      "           5       0.27      0.40      0.32       154\n",
      "           6       0.11      0.22      0.15       108\n",
      "           7       0.49      0.39      0.44       249\n",
      "           8       0.14      0.48      0.22        96\n",
      "           9       0.36      0.37      0.36       145\n",
      "          10       0.50      0.57      0.53       680\n",
      "          11       0.23      0.49      0.31       358\n",
      "          12       0.57      0.33      0.41       280\n",
      "          13       0.04      0.18      0.07        55\n",
      "          14       0.29      0.18      0.22       240\n",
      "          15       0.00      0.00      0.00        27\n",
      "          16       0.00      0.00      0.00        21\n",
      "          17       0.02      0.32      0.04        22\n",
      "          18       0.17      0.24      0.19        72\n",
      "          19       0.40      0.51      0.45        37\n",
      "          20       0.03      0.16      0.06        19\n",
      "          21       0.22      0.35      0.27       219\n",
      "          22       0.06      0.14      0.09        35\n",
      "          23       0.08      0.20      0.11        25\n",
      "          24       0.00      0.00      0.00        82\n",
      "          25       0.36      0.42      0.39        19\n",
      "          26       0.08      0.27      0.12        70\n",
      "          27       0.00      0.00      0.00        10\n",
      "          28       0.02      0.18      0.04        28\n",
      "          29       0.11      0.51      0.18        84\n",
      "          30       0.23      0.15      0.18       125\n",
      "          31       0.03      0.67      0.06        24\n",
      "          32       0.65      0.71      0.68       973\n",
      "          33       0.03      0.73      0.05        11\n",
      "          34       0.11      0.30      0.16        64\n",
      "          35       0.11      0.17      0.13        82\n",
      "          36       0.10      0.35      0.15       136\n",
      "          37       0.04      0.42      0.07        26\n",
      "          38       0.33      0.14      0.19        37\n",
      "          39       0.00      0.00      0.00        71\n",
      "          40       0.25      0.05      0.09        39\n",
      "          41       0.03      0.61      0.05        23\n",
      "\n",
      "   micro avg       0.22      0.40      0.29      5269\n",
      "   macro avg       0.17      0.28      0.17      5269\n",
      "weighted avg       0.33      0.40      0.34      5269\n",
      " samples avg       0.24      0.42      0.28      5269\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "#moc = MultiOutputClassifier(GaussianNB())                                                #f1_score: 0.34\n",
    "moc = MultiOutputClassifier(LogisticRegression(solver='lbfgs', class_weight=\"balanced\"))  #f1_score: 0.45 on reviews_rows / f1_score: 0.40\n",
    "#moc = MultiOutputClassifier(RandomForestClassifier())                                    #f1_score: 0.34\n",
    "\n",
    "moc.fit(X_train, y_train)\n",
    "y_pred = moc.predict(X_test)\n",
    "\n",
    "y_pred = moc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import Preprocessing as preproc\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Links</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Collections</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Timings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>https://www.zomato.com/hyderabad/beyond-flavou...</td>\n",
       "      <td>800</td>\n",
       "      <td>Food Hygiene Rated Restaurants in Hyderabad, C...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paradise</td>\n",
       "      <td>https://www.zomato.com/hyderabad/paradise-gach...</td>\n",
       "      <td>800</td>\n",
       "      <td>Hyderabad's Hottest</td>\n",
       "      <td>Biryani, North Indian, Chinese</td>\n",
       "      <td>11 AM to 11 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flechazo</td>\n",
       "      <td>https://www.zomato.com/hyderabad/flechazo-gach...</td>\n",
       "      <td>1,300</td>\n",
       "      <td>Great Buffets, Hyderabad's Hottest</td>\n",
       "      <td>Asian, Mediterranean, North Indian, Desserts</td>\n",
       "      <td>11:30 AM to 4:30 PM, 6:30 PM to 11 PM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Shah Ghouse Hotel &amp; Restaurant</td>\n",
       "      <td>https://www.zomato.com/hyderabad/shah-ghouse-h...</td>\n",
       "      <td>800</td>\n",
       "      <td>Late Night Restaurants</td>\n",
       "      <td>Biryani, North Indian, Chinese, Seafood, Bever...</td>\n",
       "      <td>12 Noon to 2 AM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Over The Moon Brew Company</td>\n",
       "      <td>https://www.zomato.com/hyderabad/over-the-moon...</td>\n",
       "      <td>1,200</td>\n",
       "      <td>Best Bars &amp; Pubs, Food Hygiene Rated Restauran...</td>\n",
       "      <td>Asian, Continental, North Indian, Chinese, Med...</td>\n",
       "      <td>12noon to 11pm (Mon, Tue, Wed, Thu, Sun), 12no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name  \\\n",
       "0                 Beyond Flavours   \n",
       "1                        Paradise   \n",
       "2                        Flechazo   \n",
       "3  Shah Ghouse Hotel & Restaurant   \n",
       "4      Over The Moon Brew Company   \n",
       "\n",
       "                                               Links   Cost  \\\n",
       "0  https://www.zomato.com/hyderabad/beyond-flavou...    800   \n",
       "1  https://www.zomato.com/hyderabad/paradise-gach...    800   \n",
       "2  https://www.zomato.com/hyderabad/flechazo-gach...  1,300   \n",
       "3  https://www.zomato.com/hyderabad/shah-ghouse-h...    800   \n",
       "4  https://www.zomato.com/hyderabad/over-the-moon...  1,200   \n",
       "\n",
       "                                         Collections  \\\n",
       "0  Food Hygiene Rated Restaurants in Hyderabad, C...   \n",
       "1                                Hyderabad's Hottest   \n",
       "2                 Great Buffets, Hyderabad's Hottest   \n",
       "3                             Late Night Restaurants   \n",
       "4  Best Bars & Pubs, Food Hygiene Rated Restauran...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  Chinese, Continental, Kebab, European, South I...   \n",
       "1                     Biryani, North Indian, Chinese   \n",
       "2       Asian, Mediterranean, North Indian, Desserts   \n",
       "3  Biryani, North Indian, Chinese, Seafood, Bever...   \n",
       "4  Asian, Continental, North Indian, Chinese, Med...   \n",
       "\n",
       "                                             Timings  \n",
       "0      12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)  \n",
       "1                                     11 AM to 11 PM  \n",
       "2              11:30 AM to 4:30 PM, 6:30 PM to 11 PM  \n",
       "3                                    12 Noon to 2 AM  \n",
       "4  12noon to 11pm (Mon, Tue, Wed, Thu, Sun), 12no...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants_raw = pd.read_csv(r\"data_hyderabad/105_restaurants.csv\")\n",
    "reviews_raw = pd.read_csv(r\"data_hyderabad/10k_reviews.csv\")\n",
    "\n",
    "restaurants_raw.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Angaara Counts 3',\n",
       " 'IndiBlaze',\n",
       " 'Republic Of Noodles - Lemon Tree Hotel',\n",
       " 'Sweet Basket',\n",
       " 'Wich Please'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_raw['Restaurant'].unique())\n",
    "missing_restaurants = set(restaurants_raw['Name']) - set(reviews_raw['Restaurant'])\n",
    "missing_restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Restaurant    0\n",
       "Reviewer      0\n",
       "Review        0\n",
       "Rating        0\n",
       "Metadata      0\n",
       "Time          0\n",
       "Pictures      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_data = reviews_raw[reviews_raw[\"Rating\"].notna() & reviews_raw[\"Review\"].notna()]\n",
    "reviews_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split UPPERCASE WORDS \n",
    "def splitting_words_process(word):\n",
    "    # only upper case letters\n",
    "    if word.isupper():\n",
    "        return word\n",
    "    \n",
    "    # more than one upper case letter inside\n",
    "    elif re.search(r'[A-Z][a-z]*[A-Z]', word):\n",
    "        split_word = re.findall(r'[A-Z][a-z]*', word)\n",
    "        return ' '.join(split_word)\n",
    "    \n",
    "    # <2 upper case letters\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(lambda x: ' '.join([splitting_words_process(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace 'gud', 'goo', 'gd' with the appropriate 'good'\n",
    "def replace_gud_with_good(text):\n",
    "    if isinstance(text, str):\n",
    "        # Define the regex pattern to match 'gud', 'goo', 'gd' in various capitalizations\n",
    "        pattern = re.compile(r'\\b([Gg][Uu][Dd]|[Gg][Oo][Oo]|[Gg][Dd])\\b')\n",
    "\n",
    "        # Replacement function to check the case of the first letter\n",
    "        def replacement(match):\n",
    "            word = match.group()\n",
    "            # Check if the first letter is uppercase, then return 'Good', else 'good'\n",
    "            if word[0].isupper():\n",
    "                return 'Good'\n",
    "            else:\n",
    "                return 'good'\n",
    "        \n",
    "        # Use re.sub to apply the replacement function\n",
    "        return pattern.sub(replacement, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Review' column to replace the variants of 'good'\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(replace_gud_with_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to replace 'kk', 'Oke', 'k', 'Ok' with 'ok'\n",
    "def replace_to_ok(text):\n",
    "    if isinstance(text, str):\n",
    "        # Define the regex pattern to match the variants of 'ok'\n",
    "        pattern = re.compile(r'\\b(k|kk|Ok|Oke)\\b', re.IGNORECASE)\n",
    "\n",
    "        # Replacement function to return 'ok' for all matched words\n",
    "        def replacement(match):\n",
    "            return 'ok'\n",
    "        \n",
    "        # Use re.sub to apply the replacement function\n",
    "        return pattern.sub(replacement, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply the function to the 'Review' column to replace the variants of 'ok'\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(replace_to_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add space after ! | \" | # | $ | % | & | ( | ) | * | + | , | . | : | ; followed immediately by a word\n",
    "def add_space_after_punctuation(df):\n",
    "\n",
    "    df['Review'] = df['Review'].apply(lambda text: re.sub(r'([\\u0021-\\u0026\\u0028-\\u002C\\u002E\\u003A-\\u003F]+(?=\\w))', r'\\1 ', text) if isinstance(text, str) else text)\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "reviews_data = add_space_after_punctuation(reviews_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove gibberish words like \"ggggggggggd\", \"eshjdgue\"\n",
    "def remove_gibberish(text):\n",
    "    cleaned_text = re.sub(r'\\b\\w{15,}\\b', '', text)  # removes 15+ words\n",
    "    cleaned_text = re.sub(r'\\b\\w*(\\w)\\1{2,}\\w*\\b', '', cleaned_text)  # removes words that contain 3+ repeating letters\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "reviews_data['Review'] = reviews_data['Review'].apply(remove_gibberish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace numbers with blank string\n",
    "reviews_data['Review'] = reviews_data['Review'].replace(r'\\d+(\\.\\d+)?', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = reviews_data['Review'].apply(lambda x: preproc.main_pipeline(\n",
    "    x, \n",
    "    print_output=False, \n",
    "    no_stopwords=False,\n",
    "    custom_stopwords=[],\n",
    "    convert_diacritics=True, \n",
    "    no_punctuation=False,\n",
    "    remove_contractions = True,\n",
    "    lowercase=False,\n",
    "    lemmatized=False,\n",
    "    list_pos=[\"n\",\"v\",\"a\",\"r\",\"s\"],\n",
    "    stemmed=False, \n",
    "    pos_tags_list='pos_tuples',\n",
    "    tokenized_output=False\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    punctuation_pattern = \"[\\u0021-\\u0026\\u0028-\\u002C\\u002E-\\u002F\\u003A-\\u003F\\u005B-\\u005F\\u2010-\\u2028\\ufeff`]+\"\n",
    "    stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    return [(token.lower(), pos) for token, pos in tokens if token.lower() not in stopwords and not re.match(punctuation_pattern, token)]\n",
    "\n",
    "reviews_data['Review_Preprocessed'] = preproc.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Reviewer</th>\n",
       "      <th>Review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Metadata</th>\n",
       "      <th>Time</th>\n",
       "      <th>Pictures</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Name</th>\n",
       "      <th>Links</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Collections</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>Timings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Rusha Chakraborty</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>5</td>\n",
       "      <td>1 Review , 2 Followers</td>\n",
       "      <td>5/25/2019 15:54</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>https://www.zomato.com/hyderabad/beyond-flavou...</td>\n",
       "      <td>800</td>\n",
       "      <td>Food Hygiene Rated Restaurants in Hyderabad, C...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant           Reviewer  \\\n",
       "0  Beyond Flavours  Rusha Chakraborty   \n",
       "\n",
       "                                              Review Rating  \\\n",
       "0  The ambience was good, food was quite good . h...      5   \n",
       "\n",
       "                 Metadata             Time  Pictures  \\\n",
       "0  1 Review , 2 Followers  5/25/2019 15:54         0   \n",
       "\n",
       "                                 Review_Preprocessed             Name  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...  Beyond Flavours   \n",
       "\n",
       "                                               Links Cost  \\\n",
       "0  https://www.zomato.com/hyderabad/beyond-flavou...  800   \n",
       "\n",
       "                                         Collections  \\\n",
       "0  Food Hygiene Rated Restaurants in Hyderabad, C...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  Chinese, Continental, Kebab, European, South I...   \n",
       "\n",
       "                                         Timings  \n",
       "0  12noon to 3:30pm, 6:30pm to 11:30pm (Mon-Sun)  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = pd.merge(reviews_data, restaurants_raw, left_on='Restaurant',right_on='Name', how='left')\n",
    "data_joined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_joined['Restaurant'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines  \n",
       "0  Chinese, Continental, Kebab, European, South I...  \n",
       "1  Chinese, Continental, Kebab, European, South I...  \n",
       "2  Chinese, Continental, Kebab, European, South I...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = data_joined[['Restaurant', 'Review', 'Review_Preprocessed', 'Cuisines']]\n",
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")\n",
    "model_roberta = AutoModelForTokenClassification.from_pretrained(\"Dizex/InstaFoodRoBERTa-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"ner\", model=model_roberta, tokenizer=tokenizer_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 3\u001b[0m ner_entity_results \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_joined\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msimple\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:192\u001b[0m, in \u001b[0;36mTokenClassificationPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m offset_mapping:\n\u001b[0;32m    190\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moffset_mapping\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m offset_mapping\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:1063\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1060\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1061\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1062\u001b[0m     )\n\u001b[1;32m-> 1063\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [output \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m final_iterator]\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:114\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\pt_utils.py:115\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    114\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 115\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\base.py:990\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m    989\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 990\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[0;32m    991\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:218\u001b[0m, in \u001b[0;36mTokenClassificationPipeline._forward\u001b[1;34m(self, model_inputs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(model_inputs\u001b[38;5;241m.\u001b[39mdata)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m: logits,\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecial_tokens_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: special_tokens_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs,\n\u001b[0;32m    226\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1403\u001b[0m, in \u001b[0;36mRobertaForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1398\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1401\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1403\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1413\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1415\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1417\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:846\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    837\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    839\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    840\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m    841\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m    845\u001b[0m )\n\u001b[1;32m--> 846\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    858\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    859\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:520\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    511\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    513\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    517\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 520\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:447\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    444\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    445\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 447\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    450\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\pytorch_utils.py:246\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:460\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    459\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 460\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:371\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 371\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    373\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ner_entity_results = pipe(list(data_joined['Review']), aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_entities_to_list(df: pd.DataFrame, entities: list[list[dict]]) -> pd.DataFrame:\n",
    "    def extract_entities(text, entity_list):\n",
    "        ents = []\n",
    "        for ent in entity_list:\n",
    "            e = {\"start\": ent[\"start\"], \"end\": ent[\"end\"], \"label\": ent[\"entity_group\"]}\n",
    "            if ents and (-1 <= ent[\"start\"] - ents[-1][\"end\"] <= 1) and (ents[-1][\"label\"] == e[\"label\"]):\n",
    "                ents[-1][\"end\"] = e[\"end\"]\n",
    "                continue\n",
    "            ents.append(e)\n",
    "        return [text[e[\"start\"]:e[\"end\"]] for e in ents]\n",
    "\n",
    "    df['meals'] = [extract_entities(text, entity_list) for text, entity_list in zip(df['Review'], entities)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Soumen das and Arun was a great guy. Only beca...</td>\n",
       "      <td>[(soumen, NNP), (das, NNS), (arun, NNP), (grea...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Kodi drumsticks, basket mutton biryani]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Was there for office lunch outing. Rating woul...</td>\n",
       "      <td>[(office, NN), (lunch, NN), (outing, VBG), (ra...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>I really enjoyed the follows.... The entrance,...</td>\n",
       "      <td>[(really, RB), (enjoyed, VBD), (entrance, NN),...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>I came first time in this restaurant. The entr...</td>\n",
       "      <td>[(came, VBD), (first, JJ), (time, NN), (restau...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Pathetic and horrible experience Ambience and ...</td>\n",
       "      <td>[(pathetic, JJ), (horrible, JJ), (experience, ...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ahmed was serving us. Polite and very cooperat...</td>\n",
       "      <td>[(ahmed, NNP), (serving, VBG), (us, PRP), (pol...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Veg gilaffi Kabab]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Restaurant                                             Review  \\\n",
       "0   Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1   Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2   Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "3   Beyond Flavours  Soumen das and Arun was a great guy. Only beca...   \n",
       "4   Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "..              ...                                                ...   \n",
       "95  Beyond Flavours  Was there for office lunch outing. Rating woul...   \n",
       "96  Beyond Flavours  I really enjoyed the follows.... The entrance,...   \n",
       "97  Beyond Flavours  I came first time in this restaurant. The entr...   \n",
       "98  Beyond Flavours  Pathetic and horrible experience Ambience and ...   \n",
       "99  Beyond Flavours  Ahmed was serving us. Polite and very cooperat...   \n",
       "\n",
       "                                  Review_Preprocessed  \\\n",
       "0   [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1   [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2   [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "3   [(soumen, NNP), (das, NNS), (arun, NNP), (grea...   \n",
       "4   [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "..                                                ...   \n",
       "95  [(office, NN), (lunch, NN), (outing, VBG), (ra...   \n",
       "96  [(really, RB), (enjoyed, VBD), (entrance, NN),...   \n",
       "97  [(came, VBD), (first, JJ), (time, NN), (restau...   \n",
       "98  [(pathetic, JJ), (horrible, JJ), (experience, ...   \n",
       "99  [(ahmed, NNP), (serving, VBG), (us, PRP), (pol...   \n",
       "\n",
       "                                             Cuisines  \\\n",
       "0   Chinese, Continental, Kebab, European, South I...   \n",
       "1   Chinese, Continental, Kebab, European, South I...   \n",
       "2   Chinese, Continental, Kebab, European, South I...   \n",
       "3   Chinese, Continental, Kebab, European, South I...   \n",
       "4   Chinese, Continental, Kebab, European, South I...   \n",
       "..                                                ...   \n",
       "95  Chinese, Continental, Kebab, European, South I...   \n",
       "96  Chinese, Continental, Kebab, European, South I...   \n",
       "97  Chinese, Continental, Kebab, European, South I...   \n",
       "98  Chinese, Continental, Kebab, European, South I...   \n",
       "99  Chinese, Continental, Kebab, European, South I...   \n",
       "\n",
       "                                       meals  \n",
       "0                                         []  \n",
       "1                                         []  \n",
       "2                      [Penne Alfredo Pasta]  \n",
       "3                                         []  \n",
       "4   [Kodi drumsticks, basket mutton biryani]  \n",
       "..                                       ...  \n",
       "95                                        []  \n",
       "96                                        []  \n",
       "97                                        []  \n",
       "98                                        []  \n",
       "99                       [Veg gilaffi Kabab]  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined = convert_entities_to_list(data_joined, ner_entity_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find out all the possible labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_list_into_onehot_labels(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Splits a list of comma-separated values in a specified column of a DataFrame into one-hot encoded labels.\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column_name (str): The name of the column containing comma-separated values to be one-hot encoded.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the original column split into a single column containing one-hot encoded vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    dataframe[column_name] = dataframe[column_name].apply(lambda x: x.split(\", \"))\n",
    "\n",
    "    cuisine_encoded = mlb.fit_transform(dataframe[column_name])\n",
    "\n",
    "    dataframe['Cuisine_Vector'] = list(cuisine_encoded)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[('ambience', 'NN'), ('good', 'JJ'), ('food', ...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[('ambience', 'NN'), ('good', 'JJ'), ('pleasan...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[('must', 'MD'), ('try', 'VB'), ('great', 'JJ'...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>['Penne Alfredo Pasta']</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [('ambience', 'NN'), ('good', 'JJ'), ('food', ...   \n",
       "1  [('ambience', 'NN'), ('good', 'JJ'), ('pleasan...   \n",
       "2  [('must', 'MD'), ('try', 'VB'), ('great', 'JJ'...   \n",
       "\n",
       "                                            Cuisines                    meals  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...                       []   \n",
       "1  [Chinese, Continental, Kebab, European, South ...                       []   \n",
       "2  [Chinese, Continental, Kebab, European, South ...  ['Penne Alfredo Pasta']   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data_joined = pd.read_csv(r\"data_hyderabad/data_preprocessed_classification.pkl\")\n",
    "\n",
    "#data_joined['meals'] = data_joined['meals'].apply(lambda x: [meal.lower() for meal in eval(x)])\n",
    "\n",
    "\n",
    "data_joined = split_list_into_onehot_labels(data_joined, 'Cuisines')\n",
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>lasagna, veg Platter, lasagna rolls, beers, ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>lassi, Chole bhature, Lassi, chole bhature pan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>Polite, Pan ice cream, pan ice cream, pan ice ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>ripan, politley sarvice, fish, pankaj, cake, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>ler, lamb, lamb, Noodles, rice, noodle, chilli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>noodles, Sanghai Fried Rice, Fish, sauce, nood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>mutton Haleem, Chicken Fahm Mandi, chicken hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>thukpa, spice, dimsums, chicken Gyoza, dimsums...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>chalega, Pork, beef, meat, meat, veg momo, veg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>gobi methi paratha, paratha, rice, thali, thal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Restaurant  \\\n",
       "0                       10 Downing Street   \n",
       "1                                13 Dhaba   \n",
       "2          3B's - Buddies, Bar & Barbecue   \n",
       "3               AB's - Absolute Barbecues   \n",
       "4                       Absolute Sizzlers   \n",
       "..                                    ...   \n",
       "95             Urban Asia - Kitchen & Bar   \n",
       "96  Yum Yum Tree - The Arabian Food Court   \n",
       "97        Zega - Sheraton Hyderabad Hotel   \n",
       "98               Zing's Northeast Kitchen   \n",
       "99                                eat.fit   \n",
       "\n",
       "                                               Review  \\\n",
       "0   I've been to this place about two times and i ...   \n",
       "1   I didn't go and eat at the Dhaba. I had ordere...   \n",
       "2   Gobind Passionate in serving Polite in nature ...   \n",
       "3   Excellent service by nandan and rahmat and rip...   \n",
       "4   Service was pathetic. Ordered a sizzler with l...   \n",
       "..                                                ...   \n",
       "95  This place is highly recommended. It is workin...   \n",
       "96  It is at th floor of Act Boutique building tha...   \n",
       "97  My husband and I, visited Zega for their dimsu...   \n",
       "98  After so many of goody goody excellent reviews...   \n",
       "99  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "0   've, place, two, times, really, liked, ambienc...   \n",
       "1   go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "2   gobind, passionate, serving, polite, nature, s...   \n",
       "3   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "4   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "..                                                ...   \n",
       "95  place, highly, recommended, working, eat, indi...   \n",
       "96  th, floor, act, boutique, building, entrance, ...   \n",
       "97  husband, visited, zega, dimsum, festival, disa...   \n",
       "98  many, goody, goody, excellent, reviews, n, exc...   \n",
       "99  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                              Review_Preprocessed_Pos  \\\n",
       "0   ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "1   (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "2   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "3   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "4   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "..                                                ...   \n",
       "95  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "96  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "97  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "98  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "99  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                meals  \n",
       "0   lasagna, veg Platter, lasagna rolls, beers, ve...  \n",
       "1   lassi, Chole bhature, Lassi, chole bhature pan...  \n",
       "2   Polite, Pan ice cream, pan ice cream, pan ice ...  \n",
       "3   ripan, politley sarvice, fish, pankaj, cake, b...  \n",
       "4   ler, lamb, lamb, Noodles, rice, noodle, chilli...  \n",
       "..                                                ...  \n",
       "95  noodles, Sanghai Fried Rice, Fish, sauce, nood...  \n",
       "96  mutton Haleem, Chicken Fahm Mandi, chicken hal...  \n",
       "97  thukpa, spice, dimsums, chicken Gyoza, dimsums...  \n",
       "98  chalega, Pork, beef, meat, meat, veg momo, veg...  \n",
       "99  gobi methi paratha, paratha, rice, thali, thal...  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out rows where 'meals' column is empty\n",
    "data_joined = data_joined[data_joined['meals'].apply(lambda x: x != \"[]\")]\n",
    "\n",
    "# Convert 'Review_Preprocessed' from string to list of tuples\n",
    "data_joined['Review_Preprocessed'] = data_joined['Review_Preprocessed'].apply(eval)\n",
    "\n",
    "# Extract tokens from 'Review_Preprocessed' and store in a new column\n",
    "data_joined['Review_Preprocessed_No_Pos'] = data_joined['Review_Preprocessed'].apply(lambda x: [token for token, pos in x])\n",
    "\n",
    "# Reorder columns to place 'Review_Preprocessed_No_Pos' after 'Review_Preprocessed'\n",
    "cols = list(data_joined.columns)\n",
    "review_index = cols.index('Review_Preprocessed')\n",
    "cols.insert(review_index + 1, cols.pop(cols.index('Review_Preprocessed_No_Pos')))\n",
    "data_joined = data_joined[cols]\n",
    "\n",
    "# Create a copy of the dataframe for further processing\n",
    "data_preprocessed_many_rows = data_joined.copy()\n",
    "\n",
    "# Group by 'Restaurant' and aggregate reviews and meals\n",
    "data_preprocessed_groupedby_restaurant = data_preprocessed_many_rows.groupby('Restaurant').agg({\n",
    "    'Review': lambda x: ', '.join(x),\n",
    "    'Review_Preprocessed_No_Pos': lambda x: ', '.join([', '.join(tokens) for tokens in x]),\n",
    "    'Review_Preprocessed': lambda x: ', '.join([', '.join([f\"({token}, {pos})\" for token, pos in tokens]) for tokens in x]),\n",
    "    'meals': lambda x: ', '.join([meal for sublist in x for meal in eval(sublist)])\n",
    "}).reset_index().rename(columns={\"Review_Preprocessed\": \"Review_Preprocessed_Pos\"})\n",
    "\n",
    "data_preprocessed_groupedby_restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>lasagna, veg Platter, lasagna rolls, beers, ve...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>lassi, Chole bhature, Lassi, chole bhature pan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>Polite, Pan ice cream, pan ice cream, pan ice ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>ripan, politley sarvice, fish, pankaj, cake, b...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>ler, lamb, lamb, Noodles, rice, noodle, chilli...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>noodles, Sanghai Fried Rice, Fish, sauce, nood...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>mutton Haleem, Chicken Fahm Mandi, chicken hal...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>thukpa, spice, dimsums, chicken Gyoza, dimsums...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>chalega, Pork, beef, meat, meat, veg momo, veg...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>gobi methi paratha, paratha, rice, thali, thal...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Restaurant  \\\n",
       "0                       10 Downing Street   \n",
       "1                                13 Dhaba   \n",
       "2          3B's - Buddies, Bar & Barbecue   \n",
       "3               AB's - Absolute Barbecues   \n",
       "4                       Absolute Sizzlers   \n",
       "..                                    ...   \n",
       "95             Urban Asia - Kitchen & Bar   \n",
       "96  Yum Yum Tree - The Arabian Food Court   \n",
       "97        Zega - Sheraton Hyderabad Hotel   \n",
       "98               Zing's Northeast Kitchen   \n",
       "99                                eat.fit   \n",
       "\n",
       "                                               Review  \\\n",
       "0   I've been to this place about two times and i ...   \n",
       "1   I didn't go and eat at the Dhaba. I had ordere...   \n",
       "2   Gobind Passionate in serving Polite in nature ...   \n",
       "3   Excellent service by nandan and rahmat and rip...   \n",
       "4   Service was pathetic. Ordered a sizzler with l...   \n",
       "..                                                ...   \n",
       "95  This place is highly recommended. It is workin...   \n",
       "96  It is at th floor of Act Boutique building tha...   \n",
       "97  My husband and I, visited Zega for their dimsu...   \n",
       "98  After so many of goody goody excellent reviews...   \n",
       "99  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "0   've, place, two, times, really, liked, ambienc...   \n",
       "1   go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "2   gobind, passionate, serving, polite, nature, s...   \n",
       "3   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "4   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "..                                                ...   \n",
       "95  place, highly, recommended, working, eat, indi...   \n",
       "96  th, floor, act, boutique, building, entrance, ...   \n",
       "97  husband, visited, zega, dimsum, festival, disa...   \n",
       "98  many, goody, goody, excellent, reviews, n, exc...   \n",
       "99  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                              Review_Preprocessed_Pos  \\\n",
       "0   ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "1   (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "2   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "3   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "4   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "..                                                ...   \n",
       "95  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "96  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "97  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "98  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "99  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                meals  \\\n",
       "0   lasagna, veg Platter, lasagna rolls, beers, ve...   \n",
       "1   lassi, Chole bhature, Lassi, chole bhature pan...   \n",
       "2   Polite, Pan ice cream, pan ice cream, pan ice ...   \n",
       "3   ripan, politley sarvice, fish, pankaj, cake, b...   \n",
       "4   ler, lamb, lamb, Noodles, rice, noodle, chilli...   \n",
       "..                                                ...   \n",
       "95  noodles, Sanghai Fried Rice, Fish, sauce, nood...   \n",
       "96  mutton Haleem, Chicken Fahm Mandi, chicken hal...   \n",
       "97  thukpa, spice, dimsums, chicken Gyoza, dimsums...   \n",
       "98  chalega, Pork, beef, meat, meat, veg momo, veg...   \n",
       "99  gobi methi paratha, paratha, rice, thali, thal...   \n",
       "\n",
       "                                       Cuisine_Vector  \n",
       "0   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "1   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "4   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "..                                                ...  \n",
       "95  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...  \n",
       "96  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "97  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "98  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "99  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge 'Cuisine_Vector' from 'data_joined' into 'data_preprocessed_groupedby_restaurant'\n",
    "data_preprocessed_groupedby_restaurant = data_preprocessed_groupedby_restaurant.merge(\n",
    "    data_joined[['Restaurant', 'Cuisine_Vector']],\n",
    "    on='Restaurant',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame\n",
    "data_preprocessed_groupedby_restaurant.drop_duplicates(subset=['Restaurant'], inplace=True)\n",
    "data_preprocessed_groupedby_restaurant.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preprocessed_groupedby_restaurant['meals'] = data_preprocessed_groupedby_restaurant['meals'].apply(lambda x: x.split(\", \"))\n",
    "data_preproc_grouped = data_preprocessed_groupedby_restaurant.copy()\n",
    "\n",
    "data_joined['meals'] = data_joined['meals'].apply(lambda x: [meal.lower() for meal in eval(x)])\n",
    "data_preproc_review_per_row = data_joined.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>[lasagna,, veg, Platter,, lasagna, rolls,, bee...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>[lassi,, Chole, bhature,, Lassi,, chole, bhatu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>[Polite,, Pan, ice, cream,, pan, ice, cream,, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Restaurant  \\\n",
       "0                 10 Downing Street   \n",
       "61                         13 Dhaba   \n",
       "132  3B's - Buddies, Bar & Barbecue   \n",
       "\n",
       "                                                Review  \\\n",
       "0    I've been to this place about two times and i ...   \n",
       "61   I didn't go and eat at the Dhaba. I had ordere...   \n",
       "132  Gobind Passionate in serving Polite in nature ...   \n",
       "\n",
       "                            Review_Preprocessed_No_Pos  \\\n",
       "0    've, place, two, times, really, liked, ambienc...   \n",
       "61   go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "132  gobind, passionate, serving, polite, nature, s...   \n",
       "\n",
       "                               Review_Preprocessed_Pos  \\\n",
       "0    ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "61   (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "132  (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "\n",
       "                                                 meals  \\\n",
       "0    [lasagna,, veg, Platter,, lasagna, rolls,, bee...   \n",
       "61   [lassi,, Chole, bhature,, Lassi,, chole, bhatu...   \n",
       "132  [Polite,, Pan, ice, cream,, pan, ice, cream,, ...   \n",
       "\n",
       "                                        Cuisine_Vector  \n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "61   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "132  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproc_grouped.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Word2Vec_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne alfredo pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.10267433, 0.1892341, 0.15835893, -0.5145784...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[food, good, ordered, kodi, drumsticks, basket...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi drumsticks, basket mutton biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.03495726, 0.06261446, 0.059172727, -0.38602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[well, reading, many, reviews, finally, visite...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa fish, basket biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.009220402, 0.05624652, 0.11652868, -0.53158...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "4  Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "7  Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "4  [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "7  [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \\\n",
       "2  [must, try, great, food, great, ambience, thnx...   \n",
       "4  [food, good, ordered, kodi, drumsticks, basket...   \n",
       "7  [well, reading, many, reviews, finally, visite...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "2  [Chinese, Continental, Kebab, European, South ...   \n",
       "4  [Chinese, Continental, Kebab, European, South ...   \n",
       "7  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                        meals  \\\n",
       "2                       [penne alfredo pasta]   \n",
       "4    [kodi drumsticks, basket mutton biryani]   \n",
       "7  [corn, tawa fish, basket biryani, biryani]   \n",
       "\n",
       "                                      Cuisine_Vector  \\\n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "7  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                                     Word2Vec_Vector  \n",
       "2  [0.10267433, 0.1892341, 0.15835893, -0.5145784...  \n",
       "4  [0.03495726, 0.06261446, 0.059172727, -0.38602...  \n",
       "7  [0.009220402, 0.05624652, 0.11652868, -0.53158...  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preproc_review_per_row.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Futher steps**\n",
    "\n",
    "## Final preprocessing steps:\n",
    "1. Create two datasets: \n",
    "2. 1st with 9k+ rows, so every review has a label vector\n",
    "3. 2nd with 105 rows, so every restaurant has a joint review vector of food names\n",
    "\n",
    "\n",
    "## Modeling\n",
    "I see two possible approaches to solve this problem. \n",
    "\n",
    "1. We treat the dishes as categorical instances and build a model that takes counts\n",
    "It will be a categorical to categorical model so Decision Tree or Naive Bayes should do\n",
    "\n",
    "2. Encoding the words with (no tf-idf not appropriate for this appraoch coz no documents), Word2Vec and Glove and categorizing the labels based on those. Here we can think about adding/multiplying the vectors to get the overall or dot product and using this numerical representation for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Word2Vec_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne alfredo pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.10267433, 0.1892341, 0.15835893, -0.5145784...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[food, good, ordered, kodi, drumsticks, basket...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi drumsticks, basket mutton biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.03495726, 0.06261446, 0.059172727, -0.38602...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[well, reading, many, reviews, finally, visite...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa fish, basket biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.009220402, 0.05624652, 0.11652868, -0.53158...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Came for the birthday treat of a close friend....</td>\n",
       "      <td>[(came, NN), (birthday, JJ), (treat, NN), (clo...</td>\n",
       "      <td>[came, birthday, treat, close, friend, perfect...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[chili honey lotus stem]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.015329912, 0.08770925, 0.11428657, -0.49731...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food was very good. Soup was as expected. In s...</td>\n",
       "      <td>[(food, NN), (good, JJ), (soup, NNP), (expecte...</td>\n",
       "      <td>[food, good, soup, expected, starters, ordered...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[soup, honey chilli lotus]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[0.08414163, 0.08062348, 0.10732855, -0.596129...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Restaurant                                             Review  \\\n",
       "2   Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "4   Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "7   Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "9   Beyond Flavours  Came for the birthday treat of a close friend....   \n",
       "12  Beyond Flavours  Food was very good. Soup was as expected. In s...   \n",
       "\n",
       "                                  Review_Preprocessed  \\\n",
       "2   [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "4   [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "7   [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "9   [(came, NN), (birthday, JJ), (treat, NN), (clo...   \n",
       "12  [(food, NN), (good, JJ), (soup, NNP), (expecte...   \n",
       "\n",
       "                           Review_Preprocessed_No_Pos  \\\n",
       "2   [must, try, great, food, great, ambience, thnx...   \n",
       "4   [food, good, ordered, kodi, drumsticks, basket...   \n",
       "7   [well, reading, many, reviews, finally, visite...   \n",
       "9   [came, birthday, treat, close, friend, perfect...   \n",
       "12  [food, good, soup, expected, starters, ordered...   \n",
       "\n",
       "                                             Cuisines  \\\n",
       "2   [Chinese, Continental, Kebab, European, South ...   \n",
       "4   [Chinese, Continental, Kebab, European, South ...   \n",
       "7   [Chinese, Continental, Kebab, European, South ...   \n",
       "9   [Chinese, Continental, Kebab, European, South ...   \n",
       "12  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                         meals  \\\n",
       "2                        [penne alfredo pasta]   \n",
       "4     [kodi drumsticks, basket mutton biryani]   \n",
       "7   [corn, tawa fish, basket biryani, biryani]   \n",
       "9                     [chili honey lotus stem]   \n",
       "12                  [soup, honey chilli lotus]   \n",
       "\n",
       "                                       Cuisine_Vector  \\\n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "7   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "9   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                                      Word2Vec_Vector  \n",
       "2   [0.10267433, 0.1892341, 0.15835893, -0.5145784...  \n",
       "4   [0.03495726, 0.06261446, 0.059172727, -0.38602...  \n",
       "7   [0.009220402, 0.05624652, 0.11652868, -0.53158...  \n",
       "9   [0.015329912, 0.08770925, 0.11428657, -0.49731...  \n",
       "12  [0.08414163, 0.08062348, 0.10732855, -0.596129...  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train the Word2Vec model on the 'meals' column\n",
    "reviews_skipgram_model = Word2Vec(\n",
    "    sentences=data_preproc_review_per_row[\"Review_Preprocessed_No_Pos\"].tolist(),\n",
    "    vector_size=25,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1  # Skip-gram model\n",
    ")\n",
    "\n",
    "#Generate vectors for each review\n",
    "def get_review_vector(review):\n",
    "    words = review\n",
    "    vector = sum(reviews_skipgram_model.wv[word] for word in words if word in reviews_skipgram_model.wv)\n",
    "    return vector / len(words) if words else [0] * 25\n",
    "\n",
    "# Apply the function to each review and create a new column\n",
    "data_preproc_review_per_row['Word2Vec_Vector'] = data_preproc_review_per_row['Review_Preprocessed_No_Pos'].apply(get_review_vector)\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data_preproc_review_per_row.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_Pos</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Doc2Vec_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>I've been to this place about two times and i ...</td>\n",
       "      <td>'ve, place, two, times, really, liked, ambienc...</td>\n",
       "      <td>('ve, VBP), (place, NN), (two, CD), (times, NN...</td>\n",
       "      <td>[lasagna,, veg, Platter,, lasagna, rolls,, bee...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[-0.73028654, -0.94981015, 0.8981075, -0.95641...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>I didn't go and eat at the Dhaba. I had ordere...</td>\n",
       "      <td>go, eat, dhaba, ordered, taste, amazing, te, i...</td>\n",
       "      <td>(go, VB), (eat, VB), (dhaba, NNP), (ordered, V...</td>\n",
       "      <td>[lassi,, Chole, bhature,, Lassi,, chole, bhatu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1.0386596, 1.25421, 1.8065908, -0.5012805, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>Gobind Passionate in serving Polite in nature ...</td>\n",
       "      <td>gobind, passionate, serving, polite, nature, s...</td>\n",
       "      <td>(gobind, NNP), (passionate, NNP), (serving, VB...</td>\n",
       "      <td>[Polite,, Pan, ice, cream,, pan, ice, cream,, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[-0.28862014, -1.186906, -0.28161365, -1.70050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>AB's - Absolute Barbecues</td>\n",
       "      <td>Excellent service by nandan and rahmat and rip...</td>\n",
       "      <td>excellent, service, nandan, rahmat, ripan, fee...</td>\n",
       "      <td>(excellent, JJ), (service, NN), (nandan, NN), ...</td>\n",
       "      <td>[ripan,, politley, sarvice,, fish,, pankaj,, c...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>[0.8380315, -1.8014015, 0.018671855, -2.029692...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Absolute Sizzlers</td>\n",
       "      <td>Service was pathetic. Ordered a sizzler with l...</td>\n",
       "      <td>service, pathetic, ordered, sizzler, lamb, tol...</td>\n",
       "      <td>(service, NNP), (pathetic, JJ), (ordered, VBD)...</td>\n",
       "      <td>[ler,, lamb,, lamb,, Noodles,, rice,, noodle,,...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>[0.3182445, 0.8543685, -0.23095621, -0.8876323...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5561</th>\n",
       "      <td>Urban Asia - Kitchen &amp; Bar</td>\n",
       "      <td>This place is highly recommended. It is workin...</td>\n",
       "      <td>place, highly, recommended, working, eat, indi...</td>\n",
       "      <td>(place, NN), (highly, RB), (recommended, JJ), ...</td>\n",
       "      <td>[noodles,, Sanghai, Fried, Rice,, Fish,, sauce...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.59476155, -1.0905102, -0.45617875, -0.41898...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>Yum Yum Tree - The Arabian Food Court</td>\n",
       "      <td>It is at th floor of Act Boutique building tha...</td>\n",
       "      <td>th, floor, act, boutique, building, entrance, ...</td>\n",
       "      <td>(th, JJ), (floor, NN), (act, NNP), (boutique, ...</td>\n",
       "      <td>[mutton, Haleem,, Chicken, Fahm, Mandi,, chick...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.75107616, -1.4869246, -0.94342816, -0.03576...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5700</th>\n",
       "      <td>Zega - Sheraton Hyderabad Hotel</td>\n",
       "      <td>My husband and I, visited Zega for their dimsu...</td>\n",
       "      <td>husband, visited, zega, dimsum, festival, disa...</td>\n",
       "      <td>(husband, NN), (visited, VBD), (zega, NNP), (d...</td>\n",
       "      <td>[thukpa,, spice,, dimsums,, chicken, Gyoza,, d...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.26957774, -0.03986285, -0.6722499, -0.47506...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5753</th>\n",
       "      <td>Zing's Northeast Kitchen</td>\n",
       "      <td>After so many of goody goody excellent reviews...</td>\n",
       "      <td>many, goody, goody, excellent, reviews, n, exc...</td>\n",
       "      <td>(many, JJ), (goody, NN), (goody, NN), (excelle...</td>\n",
       "      <td>[chalega,, Pork,, beef,, meat,, meat,, veg, mo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[-0.06245933, -0.1281365, -0.4542947, 0.640352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5832</th>\n",
       "      <td>eat.fit</td>\n",
       "      <td>I had ordered gobi methi paratha.. it was ok. ...</td>\n",
       "      <td>ordered, gobi, methi, paratha, ok., good, oily...</td>\n",
       "      <td>(ordered, VBN), (gobi, JJ), (methi, NN), (para...</td>\n",
       "      <td>[gobi, methi, paratha,, paratha,, rice,, thali...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>[1.4481295, 0.7667436, -0.37691307, 1.0771734,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Restaurant  \\\n",
       "0                         10 Downing Street   \n",
       "61                                 13 Dhaba   \n",
       "132          3B's - Buddies, Bar & Barbecue   \n",
       "159               AB's - Absolute Barbecues   \n",
       "193                       Absolute Sizzlers   \n",
       "...                                     ...   \n",
       "5561             Urban Asia - Kitchen & Bar   \n",
       "5627  Yum Yum Tree - The Arabian Food Court   \n",
       "5700        Zega - Sheraton Hyderabad Hotel   \n",
       "5753               Zing's Northeast Kitchen   \n",
       "5832                                eat.fit   \n",
       "\n",
       "                                                 Review  \\\n",
       "0     I've been to this place about two times and i ...   \n",
       "61    I didn't go and eat at the Dhaba. I had ordere...   \n",
       "132   Gobind Passionate in serving Polite in nature ...   \n",
       "159   Excellent service by nandan and rahmat and rip...   \n",
       "193   Service was pathetic. Ordered a sizzler with l...   \n",
       "...                                                 ...   \n",
       "5561  This place is highly recommended. It is workin...   \n",
       "5627  It is at th floor of Act Boutique building tha...   \n",
       "5700  My husband and I, visited Zega for their dimsu...   \n",
       "5753  After so many of goody goody excellent reviews...   \n",
       "5832  I had ordered gobi methi paratha.. it was ok. ...   \n",
       "\n",
       "                             Review_Preprocessed_No_Pos  \\\n",
       "0     've, place, two, times, really, liked, ambienc...   \n",
       "61    go, eat, dhaba, ordered, taste, amazing, te, i...   \n",
       "132   gobind, passionate, serving, polite, nature, s...   \n",
       "159   excellent, service, nandan, rahmat, ripan, fee...   \n",
       "193   service, pathetic, ordered, sizzler, lamb, tol...   \n",
       "...                                                 ...   \n",
       "5561  place, highly, recommended, working, eat, indi...   \n",
       "5627  th, floor, act, boutique, building, entrance, ...   \n",
       "5700  husband, visited, zega, dimsum, festival, disa...   \n",
       "5753  many, goody, goody, excellent, reviews, n, exc...   \n",
       "5832  ordered, gobi, methi, paratha, ok., good, oily...   \n",
       "\n",
       "                                Review_Preprocessed_Pos  \\\n",
       "0     ('ve, VBP), (place, NN), (two, CD), (times, NN...   \n",
       "61    (go, VB), (eat, VB), (dhaba, NNP), (ordered, V...   \n",
       "132   (gobind, NNP), (passionate, NNP), (serving, VB...   \n",
       "159   (excellent, JJ), (service, NN), (nandan, NN), ...   \n",
       "193   (service, NNP), (pathetic, JJ), (ordered, VBD)...   \n",
       "...                                                 ...   \n",
       "5561  (place, NN), (highly, RB), (recommended, JJ), ...   \n",
       "5627  (th, JJ), (floor, NN), (act, NNP), (boutique, ...   \n",
       "5700  (husband, NN), (visited, VBD), (zega, NNP), (d...   \n",
       "5753  (many, JJ), (goody, NN), (goody, NN), (excelle...   \n",
       "5832  (ordered, VBN), (gobi, JJ), (methi, NN), (para...   \n",
       "\n",
       "                                                  meals  \\\n",
       "0     [lasagna,, veg, Platter,, lasagna, rolls,, bee...   \n",
       "61    [lassi,, Chole, bhature,, Lassi,, chole, bhatu...   \n",
       "132   [Polite,, Pan, ice, cream,, pan, ice, cream,, ...   \n",
       "159   [ripan,, politley, sarvice,, fish,, pankaj,, c...   \n",
       "193   [ler,, lamb,, lamb,, Noodles,, rice,, noodle,,...   \n",
       "...                                                 ...   \n",
       "5561  [noodles,, Sanghai, Fried, Rice,, Fish,, sauce...   \n",
       "5627  [mutton, Haleem,, Chicken, Fahm, Mandi,, chick...   \n",
       "5700  [thukpa,, spice,, dimsums,, chicken, Gyoza,, d...   \n",
       "5753  [chalega,, Pork,, beef,, meat,, meat,, veg, mo...   \n",
       "5832  [gobi, methi, paratha,, paratha,, rice,, thali...   \n",
       "\n",
       "                                         Cuisine_Vector  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "61    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "132   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "159   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...   \n",
       "193   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "5561  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...   \n",
       "5627  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5700  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5753  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "5832  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, ...   \n",
       "\n",
       "                                         Doc2Vec_Vector  \n",
       "0     [-0.73028654, -0.94981015, 0.8981075, -0.95641...  \n",
       "61    [1.0386596, 1.25421, 1.8065908, -0.5012805, 0....  \n",
       "132   [-0.28862014, -1.186906, -0.28161365, -1.70050...  \n",
       "159   [0.8380315, -1.8014015, 0.018671855, -2.029692...  \n",
       "193   [0.3182445, 0.8543685, -0.23095621, -0.8876323...  \n",
       "...                                                 ...  \n",
       "5561  [0.59476155, -1.0905102, -0.45617875, -0.41898...  \n",
       "5627  [0.75107616, -1.4869246, -0.94342816, -0.03576...  \n",
       "5700  [0.26957774, -0.03986285, -0.6722499, -0.47506...  \n",
       "5753  [-0.06245933, -0.1281365, -0.4542947, 0.640352...  \n",
       "5832  [1.4481295, 0.7667436, -0.37691307, 1.0771734,...  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try Doc2Vec using the aggregated reviews\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare the data for Doc2Vec\n",
    "tagged_data = [TaggedDocument(words=review, tags=[str(i)]) for i, review in enumerate(data_preproc_grouped[\"Review_Preprocessed_No_Pos\"].tolist())]\n",
    "\n",
    "# Train the Doc2Vec model on the 'Review_Preprocessed_No_Pos' column\n",
    "doc2vec_model = Doc2Vec(\n",
    "    documents=tagged_data,\n",
    "    vector_size=25,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "# Generate vectors for each review\n",
    "def get_review_vector(doc_id):\n",
    "    return doc2vec_model.dv[doc_id]\n",
    "\n",
    "# Apply the function to each review and create a new column\n",
    "data_preproc_grouped['Doc2Vec_Vector'] = [get_review_vector(str(i)) for i in range(len(data_preproc_grouped))]\n",
    "\n",
    "# Display the DataFrame with the new column\n",
    "data_preproc_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cooccurrence_matrix_sentence_generator(preproc_sentences):\n",
    "    # Flatten the list of sentences to get all unique words\n",
    "    unique_words = list(set(word for sentence in preproc_sentences for word in sentence))\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    vocab_size = len(unique_words)\n",
    "\n",
    "    # Initialize the co-occurrence matrix\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=int)\n",
    "\n",
    "    # Compute co-occurrences\n",
    "    for sentence in tqdm(preproc_sentences):\n",
    "        word_indices = [word_index[word] for word in sentence if word in word_index]\n",
    "        for i in range(len(word_indices)):\n",
    "            co_matrix[word_indices[i], word_indices[i:]] += 1\n",
    "            co_matrix[word_indices[i:], word_indices[i]] += 1\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    co_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n",
    "\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "    # Return the co-occurrence matrix\n",
    "    return co_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CuPy GPU version:\n",
    "\n",
    "import cupy as cp\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cooccurrence_matrix_sentence_generator(preproc_sentences):\n",
    "    # Flatten the list of sentences to get all unique words\n",
    "    unique_words = list(set(word for sentence in preproc_sentences for word in sentence))\n",
    "    word_index = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    vocab_size = len(unique_words)\n",
    "\n",
    "    # Initialize the co-occurrence matrix\n",
    "    co_matrix = cp.zeros((vocab_size, vocab_size), dtype=cp.int32)\n",
    "\n",
    "    # Compute co-occurrences\n",
    "    for sentence in tqdm(preproc_sentences):\n",
    "        word_indices = [word_index[word] for word in sentence if word in word_index]\n",
    "        for i in range(len(word_indices)):\n",
    "            co_matrix[word_indices[i], word_indices[i:]] += 1\n",
    "            co_matrix[word_indices[i:], word_indices[i]] += 1\n",
    "\n",
    "    # Convert the co-occurrence matrix to a NumPy array for compatibility with pandas\n",
    "    co_matrix_np = cp.asnumpy(co_matrix)\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    co_matrix_df = pd.DataFrame(co_matrix_np, index=unique_words, columns=unique_words)\n",
    "\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=1)\n",
    "    co_matrix_df = co_matrix_df.reindex(co_matrix_df.sum().sort_values(ascending=False).index, axis=0)\n",
    "\n",
    "    # Return the co-occurrence matrix\n",
    "    return co_matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:03<00:08,  8.17it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[285], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m co_occr_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mcooccurrence_matrix_sentence_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_preproc_grouped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[236], line 19\u001b[0m, in \u001b[0;36mcooccurrence_matrix_sentence_generator\u001b[1;34m(preproc_sentences)\u001b[0m\n\u001b[0;32m     17\u001b[0m     word_indices \u001b[38;5;241m=\u001b[39m [word_index[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_index]\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(word_indices)):\n\u001b[1;32m---> 19\u001b[0m         co_matrix[word_indices[i], word_indices[i:]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     20\u001b[0m         co_matrix[word_indices[i:], word_indices[i]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for better readability\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "co_occr_matrix = cooccurrence_matrix_sentence_generator(data_preproc_grouped['meals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[239], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train GloVe model on the co-occurrence matrix\u001b[39;00m\n\u001b[0;32m      8\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m GloVe(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# 25 is the embedding dimension\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mglove_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco_occr_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Generate vectors for each review using GloVe embeddings\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_review_vector_glove\u001b[39m(review, embeddings, vocab, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\mittens_base.py:239\u001b[0m, in \u001b[0;36mGloVeBase.fit\u001b[1;34m(self, X, fixed_initialization)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, fixed_initialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124;03m\"\"\"Run GloVe and return the new matrix.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[0;32m    220\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m        of X.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mGloVeBase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixed_initialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_initialization\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\mittens_base.py:81\u001b[0m, in \u001b[0;36mMittensBase.fit\u001b[1;34m(self, X, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_dimensions(\n\u001b[0;32m     78\u001b[0m     X, vocab, initial_embedding_dict\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     80\u001b[0m weights, log_coincidence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(X)\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_coincidence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m                 \u001b[49m\u001b[43minitial_embedding_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_embedding_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mfixed_initialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_initialization\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\np_mittens.py:64\u001b[0m, in \u001b[0;36mMittens._fit\u001b[1;34m(self, coincidence, weights, log_coincidence, vocab, initial_embedding_dict, fixed_initialization)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter):\n\u001b[0;32m     63\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_prediction()\n\u001b[1;32m---> 64\u001b[0m     gradients, error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_gradients_and_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_coincidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_shapes(gradients)\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mappend(error)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\mittens\\np_mittens.py:121\u001b[0m, in \u001b[0;36mMittens._get_gradients_and_error\u001b[1;34m(self, predictions, log_coincidence, weights)\u001b[0m\n\u001b[0;32m    119\u001b[0m wgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mC)\n\u001b[0;32m    120\u001b[0m cgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW)\n\u001b[1;32m--> 121\u001b[0m bwgrad \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_diffs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    122\u001b[0m bcgrad \u001b[38;5;241m=\u001b[39m weighted_diffs\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    123\u001b[0m error \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mmultiply(weights, diffs \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\core\\generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[0;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[0;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[0;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5900\u001b[0m ):\n\u001b[0;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[1;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "from mittens import GloVe\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train GloVe model on the co-occurrence matrix\n",
    "glove_model = GloVe(n=200, max_iter=150, display_progress=10)  # 25 is the embedding dimension\n",
    "embeddings = glove_model.fit(co_occr_matrix)\n",
    "\n",
    "# Generate vectors for each review using GloVe embeddings\n",
    "def get_review_vector_glove(review, embeddings, vocab, vector_size=25):\n",
    "    words = review.split()\n",
    "    vector = np.sum([embeddings[vocab[word]] for word in words if word in vocab], axis=0)\n",
    "    return vector / len(words) if words else np.zeros(vector_size)\n",
    "\n",
    "# Apply the function to each review and create a new column\n",
    "data_preproc_review_per_row['Glove_Vector'] = data_preproc_review_per_row['Review_Preprocessed_No_Pos'].apply(lambda x: get_review_vector_glove(x, embeddings, vocab))\n",
    "\n",
    "# Assuming 'cuisine_vector' is the target column\n",
    "X = np.array(data_preproc_review_per_row['Glove_Vector'].tolist())\n",
    "y = np.array(data_preproc_review_per_row['cuisine_vector'].tolist())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the cuisine vector\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Count Vectorizer of Meals Column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[291], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m count_vector_df\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Apply the function to the 'Review_Preprocessed' column\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m data_preproc_grouped \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_preproc_grouped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeals\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m data_preproc_grouped \u001b[38;5;241m=\u001b[39m count_vectorizer(data_preproc_grouped, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview_Preprocessed_No_Pos\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m data_preprocessed_many_rows \u001b[38;5;241m=\u001b[39m count_vectorizer(data_preprocessed_many_rows, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeals\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[291], line 9\u001b[0m, in \u001b[0;36mcount_vectorizer\u001b[1;34m(df, column_name)\u001b[0m\n\u001b[0;32m      6\u001b[0m cv \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Fit and transform the specified column\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Transform the specified column\u001b[39;00m\n\u001b[0;32m     12\u001b[0m count_vector \u001b[38;5;241m=\u001b[39m cv\u001b[38;5;241m.\u001b[39mtransform(df[column_name])\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, raw_documents, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;124;03m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m \n\u001b[0;32m   1325\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1336\u001b[0m \u001b[38;5;124;03m        Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1338\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1387\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1380\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1381\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m             )\n\u001b[0;32m   1385\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1387\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1390\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1273\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1276\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 69\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def count_vectorizer(df, column_name):\n",
    "    # Initialize the CountVectorizer\n",
    "    cv = CountVectorizer()\n",
    "\n",
    "    # Fit and transform the specified column\n",
    "    cv.fit(df[column_name])\n",
    "\n",
    "    # Transform the specified column\n",
    "    count_vector = cv.transform(df[column_name])\n",
    "\n",
    "    # Create a DataFrame for better readability\n",
    "    count_vector_df = pd.DataFrame(count_vector.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "    return count_vector_df\n",
    "\n",
    "# Apply the function to the 'Review_Preprocessed' column\n",
    "data_preproc_grouped = count_vectorizer(data_preproc_grouped, 'meals')\n",
    "data_preproc_grouped = count_vectorizer(data_preproc_grouped, 'Review_Preprocessed_No_Pos')\n",
    "\n",
    "data_preprocessed_many_rows = count_vectorizer(data_preprocessed_many_rows, 'meals')\n",
    "data_preprocessed_many_rows = count_vectorizer(data_preprocessed_many_rows, 'Review_Preprocessed_No_Pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Split the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Word2Vec_Vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word2Vec_Vector'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[288], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdata_preproc_grouped\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWord2Vec_Vector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[0;32m      2\u001b[0m y \u001b[38;5;241m=\u001b[39m data_preproc_grouped[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCuisine_Vector\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_list()\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Word2Vec_Vector'"
     ]
    }
   ],
   "source": [
    "X = data_preproc_grouped['Word2Vec_Vector'].to_list()\n",
    "y = data_preproc_grouped['Cuisine_Vector'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5904, 123326)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot = OneHotEncoder()\n",
    "\n",
    "X_one_hot = data_preproc_review_per_row['reviews'].copy()\n",
    "X_one_hot = onehot.fit_transform(X).toarray()\n",
    "X_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#moc = MultiOutputClassifier(GaussianNB())                                                #f1_score: 0.34\n",
    "moc = MultiOutputClassifier(LogisticRegression(solver='lbfgs', class_weight=\"balanced\"))  #f1_score: 0.45 on reviews_rows / f1_score: 0.40\n",
    "#moc = MultiOutputClassifier(RandomForestClassifier())                                    #f1_score: 0.34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 unique labels: [0 1]\n",
      "Class 1 unique labels: [0 1]\n",
      "Class 2 unique labels: [0 1]\n",
      "Class 3 unique labels: [0 1]\n",
      "Class 4 unique labels: [0 1]\n",
      "Class 5 unique labels: [0 1]\n",
      "Class 6 unique labels: [0 1]\n",
      "Class 7 unique labels: [0 1]\n",
      "Class 8 unique labels: [0 1]\n",
      "Class 9 unique labels: [0 1]\n",
      "Class 10 unique labels: [0 1]\n",
      "Class 11 unique labels: [0 1]\n",
      "Class 12 unique labels: [0 1]\n",
      "Class 13 unique labels: [0 1]\n",
      "Class 14 unique labels: [0 1]\n",
      "Class 15 unique labels: [0 1]\n",
      "Class 16 unique labels: [0 1]\n",
      "Class 17 unique labels: [0 1]\n",
      "Class 18 unique labels: [0 1]\n",
      "Class 19 unique labels: [0 1]\n",
      "Class 20 unique labels: [0]\n",
      "Class 21 unique labels: [0 1]\n",
      "Class 22 unique labels: [0 1]\n",
      "Class 23 unique labels: [0 1]\n",
      "Class 24 unique labels: [0 1]\n",
      "Class 25 unique labels: [0]\n",
      "Class 26 unique labels: [0 1]\n",
      "Class 27 unique labels: [0 1]\n",
      "Class 28 unique labels: [0 1]\n",
      "Class 29 unique labels: [0 1]\n",
      "Class 30 unique labels: [0 1]\n",
      "Class 31 unique labels: [0 1]\n",
      "Class 32 unique labels: [0 1]\n",
      "Class 33 unique labels: [0 1]\n",
      "Class 34 unique labels: [0 1]\n",
      "Class 35 unique labels: [0 1]\n",
      "Class 36 unique labels: [0 1]\n",
      "Class 37 unique labels: [0 1]\n",
      "Class 38 unique labels: [0 1]\n",
      "Class 39 unique labels: [0 1]\n",
      "Class 40 unique labels: [0 1]\n",
      "Class 41 unique labels: [0 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.22      0.50      0.31         4\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.20      0.33      0.25         3\n",
      "           4       0.00      0.00      0.00         0\n",
      "           5       0.50      0.50      0.50         2\n",
      "           6       0.00      0.00      0.00         2\n",
      "           7       0.40      0.29      0.33         7\n",
      "           8       0.00      0.00      0.00         1\n",
      "           9       0.40      0.50      0.44         4\n",
      "          10       0.64      0.60      0.62        15\n",
      "          11       0.57      0.67      0.62         6\n",
      "          12       0.43      0.75      0.55         4\n",
      "          13       0.00      0.00      0.00         0\n",
      "          14       0.22      0.50      0.31         4\n",
      "          15       0.00      0.00      0.00         1\n",
      "          16       0.00      0.00      0.00         0\n",
      "          17       0.00      0.00      0.00         0\n",
      "          18       0.00      0.00      0.00         3\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       0.40      1.00      0.57         4\n",
      "          21       0.00      0.00      0.00         0\n",
      "          22       0.00      0.00      0.00         0\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00         0\n",
      "          25       0.00      0.00      0.00         0\n",
      "          26       0.00      0.00      0.00         0\n",
      "          27       0.00      0.00      0.00         0\n",
      "          28       0.33      0.50      0.40         2\n",
      "          29       0.00      0.00      0.00         0\n",
      "          30       0.87      0.72      0.79        18\n",
      "          31       0.00      0.00      0.00         0\n",
      "          32       0.00      0.00      0.00         0\n",
      "          33       0.00      0.00      0.00         0\n",
      "          34       0.00      0.00      0.00         4\n",
      "          35       0.00      0.00      0.00         0\n",
      "          36       0.00      0.00      0.00         0\n",
      "          37       0.00      0.00      0.00         0\n",
      "          38       0.00      0.00      0.00         1\n",
      "          39       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       0.28      0.50      0.36        88\n",
      "   macro avg       0.13      0.17      0.14        88\n",
      "weighted avg       0.46      0.50      0.46        88\n",
      " samples avg       0.34      0.50      0.38        88\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming y_train is a list of arrays, convert it to a numpy array for easier manipulation\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Check unique labels in y_train for each class\n",
    "for i in range(y_train.shape[1]):\n",
    "    unique_labels = np.unique(y_train[:, i])\n",
    "    print(f\"Class {i} unique labels: {unique_labels}\")\n",
    "\n",
    "# Remove classes with only one unique label\n",
    "valid_classes = [i for i in range(y_train.shape[1]) if len(np.unique(y_train[:, i])) > 1]\n",
    "y_train_filtered = y_train[:, valid_classes]\n",
    "\n",
    "# Train the model with filtered y_train\n",
    "moc = MultiOutputClassifier(LogisticRegression(solver='lbfgs', class_weight=\"balanced\"))\n",
    "moc.fit(X_train, y_train_filtered)\n",
    "\n",
    "# Convert y_test to a numpy array\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = moc.predict(X_test)\n",
    "print(classification_report(y_test[:, valid_classes], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

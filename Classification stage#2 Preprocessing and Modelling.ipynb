{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import Preprocessing as preproc\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_hyderabad/data_preprocessed_classification.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines                  meals  \n",
       "0  Chinese, Continental, Kebab, European, South I...                     []  \n",
       "1  Chinese, Continental, Kebab, European, South I...                     []  \n",
       "2  Chinese, Continental, Kebab, European, South I...  [Penne Alfredo Pasta]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_list_into_onehot_labels(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Splits a list of comma-separated values in a specified column of a DataFrame into one-hot encoded labels.\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column_name (str): The name of the column containing comma-separated values to be one-hot encoded.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the original column split into a single column containing one-hot encoded vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    dataframe[column_name] = dataframe[column_name].apply(lambda x: x.split(\", \"))\n",
    "\n",
    "    cuisine_encoded = mlb.fit_transform(dataframe[column_name])\n",
    "\n",
    "    dataframe['Cuisine_Vector'] = list(cuisine_encoded)\n",
    "    return dataframe, mlb\n",
    "\n",
    "data_joined, mlb = split_list_into_onehot_labels(loaded_data, 'Cuisines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines                  meals  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...                     []   \n",
       "1  [Chinese, Continental, Kebab, European, South ...                     []   \n",
       "2  [Chinese, Continental, Kebab, European, South ...  [Penne Alfredo Pasta]   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_.lower() for token in doc]\n",
    "\n",
    "def transform_and_group_dataframe(df):\n",
    "    \n",
    "    df = df[df['meals'].apply(lambda x: len(x) > 0)]  # drop the rows where there were no meals mentioned\n",
    "\n",
    "    df['Review_Preprocessed_No_Pos'] = df['Review_Preprocessed'].apply(lambda x: [token for token, pos in x])  # extract token from the tuple\n",
    "\n",
    "    # Lemmatize the text\n",
    "    df[\"Review_Preprocessed_No_Pos\"] = df['Review_Preprocessed_No_Pos'].apply(lemmatize_tokens)\n",
    "    df[\"meals\"] = df['meals'].apply(lemmatize_tokens)\n",
    "\n",
    "    data_per_row = df.reset_index(drop=True)\n",
    "\n",
    "    # Grouping\n",
    "    cuisine_df = df[['Restaurant','Cuisine_Vector']]\n",
    "\n",
    "    df = df.groupby('Restaurant').agg({\n",
    "        'Review': lambda x: [', '.join(x)],\n",
    "        'Review_Preprocessed_No_Pos': lambda x: [', '.join([', '.join(tokens) for tokens in x])],\n",
    "        'Review_Preprocessed': lambda x: [', '.join([', '.join([f\"({token}, {pos})\" for token, pos in tokens]) for tokens in x])],\n",
    "        'meals': lambda x: [', '.join([meal for sublist in x for meal in sublist])]\n",
    "    }).reset_index()\n",
    "\n",
    "    df = df.merge(cuisine_df, on='Restaurant', how='left')\n",
    "    df.drop_duplicates(subset='Restaurant', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_grouped = df\n",
    "\n",
    "    return data_grouped, data_per_row\n",
    "\n",
    "# Runs for 4 minutes\n",
    "data_grouped, data_per_row = transform_and_group_dataframe(data_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meals zostaly podzielone po spacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne, alfredo, pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi, drumstick, basket, mutton, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, order, kodi, drumstick, basket, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa, fish, basket, biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[well, read, many, review, finally, visit, amb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "1  Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "2  Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "1  [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "2  [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...   \n",
       "1  [Chinese, Continental, Kebab, European, South ...   \n",
       "2  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                          meals  \\\n",
       "0                       [penne, alfredo, pasta]   \n",
       "1    [kodi, drumstick, basket, mutton, biryani]   \n",
       "2  [corn, tawa, fish, basket, biryani, biryani]   \n",
       "\n",
       "                                      Cuisine_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \n",
       "0  [must, try, great, food, great, ambience, thnx...  \n",
       "1  [food, good, order, kodi, drumstick, basket, m...  \n",
       "2  [well, read, many, review, finally, visit, amb...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_row.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>[I've been to this place about two times and i...</td>\n",
       "      <td>[place, two, time, really, like, ambience, int...</td>\n",
       "      <td>[(place, NN), (two, CD), (times, NNS), (really...</td>\n",
       "      <td>[lasagna, veg, platter, lasagna, roll, beer, v...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>[I didn't go and eat at the Dhaba. I had order...</td>\n",
       "      <td>[go, eat, dhaba, order, taste, amazing, te, is...</td>\n",
       "      <td>[(go, VB), (eat, VB), (dhaba, NNP), (ordered, ...</td>\n",
       "      <td>[lassi, chole, bhature, lassi, chole, bhature,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>[Gobind Passionate in serving Polite in nature...</td>\n",
       "      <td>[gobind, passionate, serve, polite, nature, st...</td>\n",
       "      <td>[(gobind, NNP), (passionate, NNP), (serving, V...</td>\n",
       "      <td>[polite, pan, ice, cream, pan, ice, cream, pan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Restaurant  \\\n",
       "0               10 Downing Street   \n",
       "1                        13 Dhaba   \n",
       "2  3B's - Buddies, Bar & Barbecue   \n",
       "\n",
       "                                              Review  \\\n",
       "0  [I've been to this place about two times and i...   \n",
       "1  [I didn't go and eat at the Dhaba. I had order...   \n",
       "2  [Gobind Passionate in serving Polite in nature...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \\\n",
       "0  [place, two, time, really, like, ambience, int...   \n",
       "1  [go, eat, dhaba, order, taste, amazing, te, is...   \n",
       "2  [gobind, passionate, serve, polite, nature, st...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(place, NN), (two, CD), (times, NNS), (really...   \n",
       "1  [(go, VB), (eat, VB), (dhaba, NNP), (ordered, ...   \n",
       "2  [(gobind, NNP), (passionate, NNP), (serving, V...   \n",
       "\n",
       "                                               meals  \\\n",
       "0  [lasagna, veg, platter, lasagna, roll, beer, v...   \n",
       "1  [lassi, chole, bhature, lassi, chole, bhature,...   \n",
       "2  [polite, pan, ice, cream, pan, ice, cream, pan...   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vectorizaton**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_and_generate_vectors(dataframe, column_name, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the specified column of the dataframe and generates vectors for each entry.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the Word2Vec vectors.\n",
    "    \"\"\"\n",
    "    # Train the Word2Vec model on the specified column\n",
    "    model = Word2Vec(\n",
    "        sentences=dataframe[column_name].tolist(),\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    # Generate vectors for each entry\n",
    "    def get_vector(entry):\n",
    "        words = entry\n",
    "        vector = sum(model.wv[word] for word in words if word in model.wv)\n",
    "        return vector / len(words) if words else [0] * vector_size\n",
    "\n",
    "    # Apply the function to each entry and create a new column\n",
    "    dataframe[f'{column_name}_Word2Vec_Vector'] = dataframe[column_name].apply(get_vector)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Apply the function to the 'Review_Preprocessed_No_Pos' column\n",
    "data_per_row = train_word2vec_and_generate_vectors(data_per_row, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row = train_word2vec_and_generate_vectors(data_per_row, 'meals')\n",
    "\n",
    "data_grouped = train_word2vec_and_generate_vectors(data_grouped, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped = train_word2vec_and_generate_vectors(data_grouped, 'meals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_count_vectorizer_vector(df, column):\n",
    "  \"\"\"\n",
    "  Creates a CountVectorizer vector for each row in the specified column of the dataframe.\n",
    "  \n",
    "  Args:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column to be vectorized.\n",
    "  \n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with a new column containing the CountVectorizer vectors.\n",
    "  \"\"\"\n",
    "  # Create a CountVectorizer object\n",
    "  vectorizer = CountVectorizer()\n",
    "\n",
    "  # Fit the vectorizer on the entire column\n",
    "  vectorizer.fit([item for sublist in df[column] for item in sublist])\n",
    "\n",
    "  # Transform each row individually\n",
    "  def transform_row(row):\n",
    "    row_str = \" \".join(row)  # Convert list to string\n",
    "    X = vectorizer.transform([row_str])\n",
    "    return X.toarray()[0]\n",
    "\n",
    "  # Apply the function to each row and create a new column\n",
    "  df[f'{column}_Count_Vector'] = df[column].apply(transform_row)\n",
    "\n",
    "  return df\n",
    "\n",
    "data_per_row = create_count_vectorizer_vector(data_per_row, 'meals')\n",
    "data_per_row = create_count_vectorizer_vector(data_per_row, 'Review_Preprocessed_No_Pos')\n",
    "\n",
    "data_grouped = create_count_vectorizer_vector(data_grouped, 'meals')\n",
    "data_grouped = create_count_vectorizer_vector(data_grouped, 'Review_Preprocessed_No_Pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorize(df, column):\n",
    "    \"\"\"\n",
    "    Creates a TF-IDF vector for each row in the specified column of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column\n",
    "    vectorizer.fit([item for sublist in df[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column\n",
    "    df[f'{column}_TFIDF_Vector'] = df[column].apply(transform_row)\n",
    "\n",
    "    return df\n",
    "\n",
    "data_grouped = tfidf_vectorize(data_grouped, 'meals')\n",
    "data_grouped = tfidf_vectorize(data_grouped, 'Review_Preprocessed_No_Pos')\n",
    "\n",
    "data_per_row = tfidf_vectorize(data_per_row, 'meals')\n",
    "data_per_row = tfidf_vectorize(data_per_row, 'Review_Preprocessed_No_Pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_No_Pos_Word2Vec_Vector</th>\n",
       "      <th>meals_Word2Vec_Vector</th>\n",
       "      <th>meals_Count_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos_Count_Vector</th>\n",
       "      <th>meals_TFIDF_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos_TFIDF_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne, alfredo, pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[0.111207984, 0.32465878, 0.2145852, -0.099489...</td>\n",
       "      <td>[0.0012959279, 0.20012172, 0.17247005, 0.06112...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi, drumstick, basket, mutton, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, order, kodi, drumstick, basket, m...</td>\n",
       "      <td>[0.009134488, 0.28623053, 0.2618573, -0.027808...</td>\n",
       "      <td>[-0.12380648, 0.15835404, 0.11873293, -0.11484...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa, fish, basket, biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[well, read, many, review, finally, visit, amb...</td>\n",
       "      <td>[0.03234672, 0.27808735, 0.26538435, -0.096172...</td>\n",
       "      <td>[-0.08470791, 0.14191578, 0.14373763, -0.12358...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Came for the birthday treat of a close friend....</td>\n",
       "      <td>[(came, NN), (birthday, JJ), (treat, NN), (clo...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[chili, honey, lotus, stem]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[come, birthday, treat, close, friend, perfect...</td>\n",
       "      <td>[0.10670576, 0.2354382, 0.19815248, -0.0926924...</td>\n",
       "      <td>[-0.09642626, 0.14300913, 0.12458401, -0.09023...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food was very good. Soup was as expected. In s...</td>\n",
       "      <td>[(food, NN), (good, JJ), (soup, NNP), (expecte...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[soup, honey, chilli, lotus]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, soup, expect, starter, order, hon...</td>\n",
       "      <td>[0.05089178, 0.26557732, 0.25723594, 0.0086472...</td>\n",
       "      <td>[-0.082653165, 0.103749424, 0.12061235, -0.023...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "1  Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "2  Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "3  Beyond Flavours  Came for the birthday treat of a close friend....   \n",
       "4  Beyond Flavours  Food was very good. Soup was as expected. In s...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "1  [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "2  [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "3  [(came, NN), (birthday, JJ), (treat, NN), (clo...   \n",
       "4  [(food, NN), (good, JJ), (soup, NNP), (expecte...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...   \n",
       "1  [Chinese, Continental, Kebab, European, South ...   \n",
       "2  [Chinese, Continental, Kebab, European, South ...   \n",
       "3  [Chinese, Continental, Kebab, European, South ...   \n",
       "4  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                          meals  \\\n",
       "0                       [penne, alfredo, pasta]   \n",
       "1    [kodi, drumstick, basket, mutton, biryani]   \n",
       "2  [corn, tawa, fish, basket, biryani, biryani]   \n",
       "3                   [chili, honey, lotus, stem]   \n",
       "4                  [soup, honey, chilli, lotus]   \n",
       "\n",
       "                                      Cuisine_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \\\n",
       "0  [must, try, great, food, great, ambience, thnx...   \n",
       "1  [food, good, order, kodi, drumstick, basket, m...   \n",
       "2  [well, read, many, review, finally, visit, amb...   \n",
       "3  [come, birthday, treat, close, friend, perfect...   \n",
       "4  [food, good, soup, expect, starter, order, hon...   \n",
       "\n",
       "          Review_Preprocessed_No_Pos_Word2Vec_Vector  \\\n",
       "0  [0.111207984, 0.32465878, 0.2145852, -0.099489...   \n",
       "1  [0.009134488, 0.28623053, 0.2618573, -0.027808...   \n",
       "2  [0.03234672, 0.27808735, 0.26538435, -0.096172...   \n",
       "3  [0.10670576, 0.2354382, 0.19815248, -0.0926924...   \n",
       "4  [0.05089178, 0.26557732, 0.25723594, 0.0086472...   \n",
       "\n",
       "                               meals_Word2Vec_Vector  \\\n",
       "0  [0.0012959279, 0.20012172, 0.17247005, 0.06112...   \n",
       "1  [-0.12380648, 0.15835404, 0.11873293, -0.11484...   \n",
       "2  [-0.08470791, 0.14191578, 0.14373763, -0.12358...   \n",
       "3  [-0.09642626, 0.14300913, 0.12458401, -0.09023...   \n",
       "4  [-0.082653165, 0.103749424, 0.12061235, -0.023...   \n",
       "\n",
       "                                  meals_Count_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "             Review_Preprocessed_No_Pos_Count_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  meals_TFIDF_Vector  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "             Review_Preprocessed_No_Pos_TFIDF_Vector  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_row.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling without data leakage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data_per_row_train, data_per_row_test = train_test_split(data_per_row, test_size=0.2, random_state=42, shuffle=True)\n",
    "data_grouped_train, data_grouped_test = train_test_split(data_grouped, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_and_generate_vectors(dataframe_train, dataframe_test, column_name, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the specified column of the training dataframe and generates vectors for both train and test data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the Word2Vec vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the Word2Vec model on the training data\n",
    "    model = Word2Vec(\n",
    "        sentences=dataframe_train[column_name].tolist(),\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    # Function to generate vectors for a given sentence\n",
    "    def get_vector(entry):\n",
    "        words = entry\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "            else:\n",
    "                # 2. Zero Vector:\n",
    "                 vectors.append(np.zeros(vector_size))\n",
    "        if vectors:\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "\n",
    "    # Apply the function to both train and test dataframes\n",
    "    dataframe_train[f'{column_name}_Word2Vec_Vector'] = dataframe_train[column_name].apply(get_vector)\n",
    "    dataframe_test[f'{column_name}_Word2Vec_Vector'] = dataframe_test[column_name].apply(get_vector)\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "# Apply the function to the 'Review_Preprocessed_No_Pos' column\n",
    "data_per_row_train, data_per_row_test = train_word2vec_and_generate_vectors(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = train_word2vec_and_generate_vectors(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = train_word2vec_and_generate_vectors(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = train_word2vec_and_generate_vectors(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_count_vectorizer_vector(dataframe_train, dataframe_test, column):\n",
    "    \"\"\"\n",
    "    Creates a CountVectorizer vector for each row in the specified column of the training and testing dataframes.\n",
    "    \n",
    "    Args:\n",
    "        dataframe_train (pd.DataFrame): The training DataFrame containing the data.\n",
    "        dataframe_test (pd.DataFrame): The testing DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: The training and testing DataFrames with new columns containing the CountVectorizer vectors.\n",
    "    \"\"\"\n",
    "    # Create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column of the training dataframe\n",
    "    vectorizer.fit([item for sublist in dataframe_train[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually for the training dataframe\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column for the training dataframe\n",
    "    dataframe_train[f'{column}_Count_Vector'] = dataframe_train[column].apply(transform_row)\n",
    "    # Apply the function to each row and create a new column for the testing dataframe\n",
    "    dataframe_test[f'{column}_Count_Vector'] = dataframe_test[column].apply(transform_row)\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "data_per_row_train, data_per_row_test = create_count_vectorizer_vector(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = create_count_vectorizer_vector(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = create_count_vectorizer_vector(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = create_count_vectorizer_vector(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorize(dataframe_train, dataframe_test, column):\n",
    "    \"\"\"\n",
    "    Creates a TF-IDF vector for each row in the specified column of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column\n",
    "    vectorizer.fit([item for sublist in dataframe_train[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column\n",
    "    dataframe_train[f'{column}_TFIDF_Vector'] = dataframe_train[column].apply(transform_row)\n",
    "    dataframe_test[f'{column}_TFIDF_Vector'] = dataframe_test[column].apply(transform_row)\n",
    "\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "data_per_row_train, data_per_row_test = tfidf_vectorize(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = tfidf_vectorize(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = tfidf_vectorize(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = tfidf_vectorize(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        'macro_f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "def evaluate_single_model(feature_col, base_model, wrapper, df_type, train_df, test_df, target_col):\n",
    "    try:\n",
    "        # Prepare data\n",
    "        X_train = np.vstack(train_df[feature_col].values)\n",
    "        X_test = np.vstack(test_df[feature_col].values)\n",
    "        y_train = np.array(train_df[target_col].tolist())\n",
    "        y_test = np.array(test_df[target_col].tolist())\n",
    "        \n",
    "        # Quick check for class distribution\n",
    "        #if len(np.unique(y_train)) < 2:\n",
    "        #    return None\n",
    "        \n",
    "        # Create and train model\n",
    "        model = wrapper(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluate_model(y_test, y_pred, f\"{wrapper.__name__}_{base_model.__class__.__name__}_{feature_col}\")\n",
    "        eval_result['feature_col'] = feature_col\n",
    "        eval_result['df_type'] = df_type\n",
    "        return eval_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {wrapper.__name__}_{base_model.__class__.__name__}_{feature_col} on {df_type}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def train_evaluate_models(df_train_per_row, df_test_per_row, \n",
    "                         df_train_grouped, df_test_grouped, \n",
    "                         feature_cols, target_col, n_jobs=-1):\n",
    "    \"\"\"Parallel training and evaluation of models\"\"\"\n",
    "    \n",
    "    # Define base models - initialize once\n",
    "    base_models = {\n",
    "        'LogisticRegression': LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\", max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "        'LinearSVC': LinearSVC(random_state=0, max_iter=2000)\n",
    "    }\n",
    "    \n",
    "    # Define wrapper models\n",
    "    wrappers = {\n",
    "        'ClassifierChain': ClassifierChain,\n",
    "        'OneVsRest': OneVsRestClassifier,\n",
    "        'MultiOutput': MultiOutputClassifier\n",
    "    }\n",
    "    \n",
    "    # Define DataFrame types\n",
    "    df_types = {\n",
    "        'per_row': (df_train_per_row, df_test_per_row),\n",
    "        'grouped': (df_train_grouped, df_test_grouped)\n",
    "    }\n",
    "    \n",
    "    # Create all combinations\n",
    "    combinations = [\n",
    "        (feature_col, base_model, wrapper, df_type, train_df, test_df)\n",
    "        for feature_col in feature_cols\n",
    "        for base_model in base_models.values()\n",
    "        for wrapper in wrappers.values()\n",
    "        for df_type, (train_df, test_df) in df_types.items()\n",
    "    ]\n",
    "    \n",
    "    # Run parallel evaluation\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_single_model)(\n",
    "            feature_col, base_model, wrapper, df_type, train_df, test_df, target_col\n",
    "        )\n",
    "        for feature_col, base_model, wrapper, df_type, train_df, test_df in tqdm(combinations)\n",
    "    )\n",
    "    \n",
    "    # Filter None results and convert to DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "feature_columns = [\n",
    "    'meals_Word2Vec_Vector',\n",
    "    'Review_Preprocessed_No_Pos_Word2Vec_Vector',\n",
    "\n",
    "    'meals_Count_Vector',    \n",
    "    'Review_Preprocessed_No_Pos_Count_Vector',\n",
    "\n",
    "    'meals_TFIDF_Vector',    \n",
    "    'Review_Preprocessed_No_Pos_TFIDF_Vector'\n",
    "]\n",
    "\n",
    "# Usage remains the same\n",
    "results = train_evaluate_models(\n",
    "    df_train_per_row=data_per_row_train,\n",
    "    df_test_per_row=data_per_row_test,\n",
    "    df_train_grouped=data_grouped_train,\n",
    "    df_test_grouped=data_grouped_test,\n",
    "    feature_cols=feature_columns,\n",
    "    target_col='Cuisine_Vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If called again, pleases change the name of the file \n",
    "\n",
    "#results.to_csv('data_hyderabad/all_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mFalse\u001b[39;00m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results.sort_values(['macro_f1'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Indian</td>\n",
       "      <td>0.829157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Lebanese</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>North Eastern</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Desserts</td>\n",
       "      <td>0.706173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ice Cream</td>\n",
       "      <td>0.705882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Healthy Food</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.693750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Pizza</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bakery</td>\n",
       "      <td>0.652632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Continental</td>\n",
       "      <td>0.640708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Fast Food</td>\n",
       "      <td>0.639118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Burger</td>\n",
       "      <td>0.614286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Cafe</td>\n",
       "      <td>0.600858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>American</td>\n",
       "      <td>0.590909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Biryani</td>\n",
       "      <td>0.575540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asian</td>\n",
       "      <td>0.552239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Momos</td>\n",
       "      <td>0.550336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Beverages</td>\n",
       "      <td>0.549708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Italian</td>\n",
       "      <td>0.548813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Mediterranean</td>\n",
       "      <td>0.539007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BBQ</td>\n",
       "      <td>0.524590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Wraps</td>\n",
       "      <td>0.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Goan</td>\n",
       "      <td>0.482759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>European</td>\n",
       "      <td>0.481481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>0.478261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Modern Indian</td>\n",
       "      <td>0.472727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Street Food</td>\n",
       "      <td>0.461538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Mughlai</td>\n",
       "      <td>0.429268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andhra</td>\n",
       "      <td>0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Seafood</td>\n",
       "      <td>0.367816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Thai</td>\n",
       "      <td>0.358974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Kebab</td>\n",
       "      <td>0.358696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hyderabadi</td>\n",
       "      <td>0.353741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>South Indian</td>\n",
       "      <td>0.353414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arabian</td>\n",
       "      <td>0.321839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Sushi</td>\n",
       "      <td>0.317241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Salad</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Finger Food</td>\n",
       "      <td>0.258065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Indonesian</td>\n",
       "      <td>0.256410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Mexican</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Juices</td>\n",
       "      <td>0.074074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Label  F1-Score\n",
       "32   North Indian  0.829157\n",
       "25       Lebanese  0.722222\n",
       "31  North Eastern  0.720000\n",
       "12       Desserts  0.706173\n",
       "19      Ice Cream  0.705882\n",
       "17   Healthy Food  0.700000\n",
       "10        Chinese  0.693750\n",
       "33          Pizza  0.666667\n",
       "5          Bakery  0.652632\n",
       "11    Continental  0.640708\n",
       "14      Fast Food  0.639118\n",
       "8          Burger  0.614286\n",
       "9            Cafe  0.600858\n",
       "0        American  0.590909\n",
       "7         Biryani  0.575540\n",
       "3           Asian  0.552239\n",
       "29          Momos  0.550336\n",
       "6       Beverages  0.549708\n",
       "21        Italian  0.548813\n",
       "26  Mediterranean  0.539007\n",
       "4             BBQ  0.524590\n",
       "41          Wraps  0.510638\n",
       "16           Goan  0.482759\n",
       "13       European  0.481481\n",
       "37        Spanish  0.478261\n",
       "28  Modern Indian  0.472727\n",
       "38    Street Food  0.461538\n",
       "30        Mughlai  0.429268\n",
       "1          Andhra  0.410000\n",
       "35        Seafood  0.367816\n",
       "40           Thai  0.358974\n",
       "24          Kebab  0.358696\n",
       "18     Hyderabadi  0.353741\n",
       "36   South Indian  0.353414\n",
       "2         Arabian  0.321839\n",
       "39          Sushi  0.317241\n",
       "34          Salad  0.304000\n",
       "15    Finger Food  0.258065\n",
       "20     Indonesian  0.256410\n",
       "22       Japanese  0.222222\n",
       "27        Mexican  0.105263\n",
       "23         Juices  0.074074"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "X_train = np.vstack(data_per_row_train[\"Review_Preprocessed_No_Pos_TFIDF_Vector\"].values)\n",
    "X_test = np.vstack(data_per_row_test[\"Review_Preprocessed_No_Pos_TFIDF_Vector\"].values)\n",
    "y_train = np.array(data_per_row_train[\"Cuisine_Vector\"].tolist())\n",
    "y_test = np.array(data_per_row_test[\"Cuisine_Vector\"].tolist())\n",
    "\n",
    "moc = MultiOutputClassifier(LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\")).fit(X_train, y_train)\n",
    "y_pred = moc.predict(X_test)\n",
    "\n",
    "report = classification_report(y_test, y_pred, target_names=mlb.classes_, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "\n",
    "predicted_labels_df = report_df.loc[mlb.classes_, ['f1-score']].reset_index()\n",
    "predicted_labels_df.columns = ['Label', 'F1-Score']\n",
    "predicted_labels_df = predicted_labels_df.sort_values('F1-Score', ascending=False)\n",
    "predicted_labels_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

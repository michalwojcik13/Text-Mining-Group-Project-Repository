{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\wojci\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import emoji\n",
    "import Preprocessing as preproc\n",
    "import nltk\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data_hyderabad/data_preprocessed_classification.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>Chinese, Continental, Kebab, European, South I...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines                  meals  \n",
       "0  Chinese, Continental, Kebab, European, South I...                     []  \n",
       "1  Chinese, Continental, Kebab, European, South I...                     []  \n",
       "2  Chinese, Continental, Kebab, European, South I...  [Penne Alfredo Pasta]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def split_list_into_onehot_labels(dataframe, column_name):\n",
    "    \"\"\"\n",
    "    Splits a list of comma-separated values in a specified column of a DataFrame into one-hot encoded labels.\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column_name (str): The name of the column containing comma-separated values to be one-hot encoded.\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the original column split into a single column containing one-hot encoded vectors.\n",
    "\n",
    "    \"\"\"\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    dataframe[column_name] = dataframe[column_name].apply(lambda x: x.split(\", \"))\n",
    "\n",
    "    cuisine_encoded = mlb.fit_transform(dataframe[column_name])\n",
    "\n",
    "    dataframe['Cuisine_Vector'] = list(cuisine_encoded)\n",
    "    return dataframe\n",
    "\n",
    "data_joined = split_list_into_onehot_labels(loaded_data, 'Cuisines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>The ambience was good, food was quite good . h...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (food, NN), (quit...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Ambience is too good for a pleasant evening. S...</td>\n",
       "      <td>[(ambience, NN), (good, JJ), (pleasant, JJ), (...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[Penne Alfredo Pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  The ambience was good, food was quite good . h...   \n",
       "1  Beyond Flavours  Ambience is too good for a pleasant evening. S...   \n",
       "2  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(ambience, NN), (good, JJ), (food, NN), (quit...   \n",
       "1  [(ambience, NN), (good, JJ), (pleasant, JJ), (...   \n",
       "2  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "\n",
       "                                            Cuisines                  meals  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...                     []   \n",
       "1  [Chinese, Continental, Kebab, European, South ...                     []   \n",
       "2  [Chinese, Continental, Kebab, European, South ...  [Penne Alfredo Pasta]   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_joined.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_.lower() for token in doc]\n",
    "\n",
    "def transform_and_group_dataframe(df):\n",
    "    \n",
    "    df = df[df['meals'].apply(lambda x: len(x) > 0)]  # drop the rows where there were no meals mentioned\n",
    "\n",
    "    df['Review_Preprocessed_No_Pos'] = df['Review_Preprocessed'].apply(lambda x: [token for token, pos in x])  # extract token from the tuple\n",
    "\n",
    "    # Lemmatize the text\n",
    "    df[\"Review_Preprocessed_No_Pos\"] = df['Review_Preprocessed_No_Pos'].apply(lemmatize_tokens)\n",
    "    df[\"meals\"] = df['meals'].apply(lemmatize_tokens)\n",
    "\n",
    "    data_per_row = df.reset_index(drop=True)\n",
    "\n",
    "    # Grouping\n",
    "    cuisine_df = df[['Restaurant','Cuisine_Vector']]\n",
    "\n",
    "    df = df.groupby('Restaurant').agg({\n",
    "        'Review': lambda x: [', '.join(x)],\n",
    "        'Review_Preprocessed_No_Pos': lambda x: [', '.join([', '.join(tokens) for tokens in x])],\n",
    "        'Review_Preprocessed': lambda x: [', '.join([', '.join([f\"({token}, {pos})\" for token, pos in tokens]) for tokens in x])],\n",
    "        'meals': lambda x: [', '.join([meal for sublist in x for meal in sublist])]\n",
    "    }).reset_index()\n",
    "\n",
    "    df = df.merge(cuisine_df, on='Restaurant', how='left')\n",
    "    df.drop_duplicates(subset='Restaurant', inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_grouped = df\n",
    "\n",
    "    return data_grouped, data_per_row\n",
    "\n",
    "# Runs for 4 minutes\n",
    "data_grouped, data_per_row = transform_and_group_dataframe(data_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meals zostaly podzielone po spacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne, alfredo, pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi, drumstick, basket, mutton, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, order, kodi, drumstick, basket, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa, fish, basket, biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[well, read, many, review, finally, visit, amb...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "1  Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "2  Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "1  [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "2  [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...   \n",
       "1  [Chinese, Continental, Kebab, European, South ...   \n",
       "2  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                          meals  \\\n",
       "0                       [penne, alfredo, pasta]   \n",
       "1    [kodi, drumstick, basket, mutton, biryani]   \n",
       "2  [corn, tawa, fish, basket, biryani, biryani]   \n",
       "\n",
       "                                      Cuisine_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \n",
       "0  [must, try, great, food, great, ambience, thnx...  \n",
       "1  [food, good, order, kodi, drumstick, basket, m...  \n",
       "2  [well, read, many, review, finally, visit, amb...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_row.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10 Downing Street</td>\n",
       "      <td>[I've been to this place about two times and i...</td>\n",
       "      <td>[place, two, time, really, like, ambience, int...</td>\n",
       "      <td>[(place, NN), (two, CD), (times, NNS), (really...</td>\n",
       "      <td>[lasagna, veg, platter, lasagna, roll, beer, v...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13 Dhaba</td>\n",
       "      <td>[I didn't go and eat at the Dhaba. I had order...</td>\n",
       "      <td>[go, eat, dhaba, order, taste, amazing, te, is...</td>\n",
       "      <td>[(go, VB), (eat, VB), (dhaba, NNP), (ordered, ...</td>\n",
       "      <td>[lassi, chole, bhature, lassi, chole, bhature,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3B's - Buddies, Bar &amp; Barbecue</td>\n",
       "      <td>[Gobind Passionate in serving Polite in nature...</td>\n",
       "      <td>[gobind, passionate, serve, polite, nature, st...</td>\n",
       "      <td>[(gobind, NNP), (passionate, NNP), (serving, V...</td>\n",
       "      <td>[polite, pan, ice, cream, pan, ice, cream, pan...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Restaurant  \\\n",
       "0               10 Downing Street   \n",
       "1                        13 Dhaba   \n",
       "2  3B's - Buddies, Bar & Barbecue   \n",
       "\n",
       "                                              Review  \\\n",
       "0  [I've been to this place about two times and i...   \n",
       "1  [I didn't go and eat at the Dhaba. I had order...   \n",
       "2  [Gobind Passionate in serving Polite in nature...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \\\n",
       "0  [place, two, time, really, like, ambience, int...   \n",
       "1  [go, eat, dhaba, order, taste, amazing, te, is...   \n",
       "2  [gobind, passionate, serve, polite, nature, st...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(place, NN), (two, CD), (times, NNS), (really...   \n",
       "1  [(go, VB), (eat, VB), (dhaba, NNP), (ordered, ...   \n",
       "2  [(gobind, NNP), (passionate, NNP), (serving, V...   \n",
       "\n",
       "                                               meals  \\\n",
       "0  [lasagna, veg, platter, lasagna, roll, beer, v...   \n",
       "1  [lassi, chole, bhature, lassi, chole, bhature,...   \n",
       "2  [polite, pan, ice, cream, pan, ice, cream, pan...   \n",
       "\n",
       "                                      Cuisine_Vector  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_grouped.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Vectorizaton**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_and_generate_vectors(dataframe, column_name, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the specified column of the dataframe and generates vectors for each entry.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the Word2Vec vectors.\n",
    "    \"\"\"\n",
    "    # Train the Word2Vec model on the specified column\n",
    "    model = Word2Vec(\n",
    "        sentences=dataframe[column_name].tolist(),\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    # Generate vectors for each entry\n",
    "    def get_vector(entry):\n",
    "        words = entry\n",
    "        vector = sum(model.wv[word] for word in words if word in model.wv)\n",
    "        return vector / len(words) if words else [0] * vector_size\n",
    "\n",
    "    # Apply the function to each entry and create a new column\n",
    "    dataframe[f'{column_name}_Word2Vec_Vector'] = dataframe[column_name].apply(get_vector)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "# Apply the function to the 'Review_Preprocessed_No_Pos' column\n",
    "data_per_row = train_word2vec_and_generate_vectors(data_per_row, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row = train_word2vec_and_generate_vectors(data_per_row, 'meals')\n",
    "\n",
    "data_grouped = train_word2vec_and_generate_vectors(data_grouped, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped = train_word2vec_and_generate_vectors(data_grouped, 'meals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_count_vectorizer_vector(df, column):\n",
    "  \"\"\"\n",
    "  Creates a CountVectorizer vector for each row in the specified column of the dataframe.\n",
    "  \n",
    "  Args:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    column (str): The name of the column to be vectorized.\n",
    "  \n",
    "  Returns:\n",
    "    pd.DataFrame: The DataFrame with a new column containing the CountVectorizer vectors.\n",
    "  \"\"\"\n",
    "  # Create a CountVectorizer object\n",
    "  vectorizer = CountVectorizer()\n",
    "\n",
    "  # Fit the vectorizer on the entire column\n",
    "  vectorizer.fit([item for sublist in df[column] for item in sublist])\n",
    "\n",
    "  # Transform each row individually\n",
    "  def transform_row(row):\n",
    "    row_str = \" \".join(row)  # Convert list to string\n",
    "    X = vectorizer.transform([row_str])\n",
    "    return X.toarray()[0]\n",
    "\n",
    "  # Apply the function to each row and create a new column\n",
    "  df[f'{column}_Count_Vector'] = df[column].apply(transform_row)\n",
    "\n",
    "  return df\n",
    "\n",
    "data_per_row = create_count_vectorizer_vector(data_per_row, 'meals')\n",
    "data_per_row = create_count_vectorizer_vector(data_per_row, 'Review_Preprocessed_No_Pos')\n",
    "\n",
    "data_grouped = create_count_vectorizer_vector(data_grouped, 'meals')\n",
    "data_grouped = create_count_vectorizer_vector(data_grouped, 'Review_Preprocessed_No_Pos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorize(df, column):\n",
    "    \"\"\"\n",
    "    Creates a TF-IDF vector for each row in the specified column of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column\n",
    "    vectorizer.fit([item for sublist in df[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column\n",
    "    df[f'{column}_TFIDF_Vector'] = df[column].apply(transform_row)\n",
    "\n",
    "    return df\n",
    "\n",
    "data_grouped = tfidf_vectorize(data_grouped, 'meals')\n",
    "data_grouped = tfidf_vectorize(data_grouped, 'Review_Preprocessed_No_Pos')\n",
    "\n",
    "data_per_row = tfidf_vectorize(data_per_row, 'meals')\n",
    "data_per_row = tfidf_vectorize(data_per_row, 'Review_Preprocessed_No_Pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Restaurant</th>\n",
       "      <th>Review</th>\n",
       "      <th>Review_Preprocessed</th>\n",
       "      <th>Cuisines</th>\n",
       "      <th>meals</th>\n",
       "      <th>Cuisine_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos</th>\n",
       "      <th>Review_Preprocessed_No_Pos_Word2Vec_Vector</th>\n",
       "      <th>meals_Word2Vec_Vector</th>\n",
       "      <th>meals_Count_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos_Count_Vector</th>\n",
       "      <th>meals_TFIDF_Vector</th>\n",
       "      <th>Review_Preprocessed_No_Pos_TFIDF_Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>A must try.. great food great ambience. Thnx f...</td>\n",
       "      <td>[(must, MD), (try, VB), (great, JJ), (food, NN...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[penne, alfredo, pasta]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[must, try, great, food, great, ambience, thnx...</td>\n",
       "      <td>[0.17076543, 0.25489318, 0.13064787, -0.029729...</td>\n",
       "      <td>[-0.00069748174, 0.22289085, 0.1799066, 0.0503...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food is good. we ordered Kodi drumsticks and b...</td>\n",
       "      <td>[(food, NN), (good, JJ), (ordered, VBD), (kodi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[kodi, drumstick, basket, mutton, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, order, kodi, drumstick, basket, m...</td>\n",
       "      <td>[0.040961727, 0.27078453, 0.20895234, 0.050825...</td>\n",
       "      <td>[-0.14319071, 0.1444223, 0.10494959, -0.111831...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Well after reading so many reviews finally vis...</td>\n",
       "      <td>[(well, RB), (reading, VBG), (many, JJ), (revi...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[corn, tawa, fish, basket, biryani, biryani]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[well, read, many, review, finally, visit, amb...</td>\n",
       "      <td>[0.05618261, 0.20712899, 0.18163988, -0.000625...</td>\n",
       "      <td>[-0.11835837, 0.1308477, 0.1284879, -0.1177104...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Came for the birthday treat of a close friend....</td>\n",
       "      <td>[(came, NN), (birthday, JJ), (treat, NN), (clo...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[chili, honey, lotus, stem]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[come, birthday, treat, close, friend, perfect...</td>\n",
       "      <td>[0.12596513, 0.18433549, 0.12829378, -0.007487...</td>\n",
       "      <td>[-0.11019059, 0.1285018, 0.11618678, -0.114508...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Beyond Flavours</td>\n",
       "      <td>Food was very good. Soup was as expected. In s...</td>\n",
       "      <td>[(food, NN), (good, JJ), (soup, NNP), (expecte...</td>\n",
       "      <td>[Chinese, Continental, Kebab, European, South ...</td>\n",
       "      <td>[soup, honey, chilli, lotus]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...</td>\n",
       "      <td>[food, good, soup, expect, starter, order, hon...</td>\n",
       "      <td>[0.055219177, 0.18505082, 0.16418888, 0.081227...</td>\n",
       "      <td>[-0.1173533, 0.10339392, 0.12608914, -0.057435...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Restaurant                                             Review  \\\n",
       "0  Beyond Flavours  A must try.. great food great ambience. Thnx f...   \n",
       "1  Beyond Flavours  Food is good. we ordered Kodi drumsticks and b...   \n",
       "2  Beyond Flavours  Well after reading so many reviews finally vis...   \n",
       "3  Beyond Flavours  Came for the birthday treat of a close friend....   \n",
       "4  Beyond Flavours  Food was very good. Soup was as expected. In s...   \n",
       "\n",
       "                                 Review_Preprocessed  \\\n",
       "0  [(must, MD), (try, VB), (great, JJ), (food, NN...   \n",
       "1  [(food, NN), (good, JJ), (ordered, VBD), (kodi...   \n",
       "2  [(well, RB), (reading, VBG), (many, JJ), (revi...   \n",
       "3  [(came, NN), (birthday, JJ), (treat, NN), (clo...   \n",
       "4  [(food, NN), (good, JJ), (soup, NNP), (expecte...   \n",
       "\n",
       "                                            Cuisines  \\\n",
       "0  [Chinese, Continental, Kebab, European, South ...   \n",
       "1  [Chinese, Continental, Kebab, European, South ...   \n",
       "2  [Chinese, Continental, Kebab, European, South ...   \n",
       "3  [Chinese, Continental, Kebab, European, South ...   \n",
       "4  [Chinese, Continental, Kebab, European, South ...   \n",
       "\n",
       "                                          meals  \\\n",
       "0                       [penne, alfredo, pasta]   \n",
       "1    [kodi, drumstick, basket, mutton, biryani]   \n",
       "2  [corn, tawa, fish, basket, biryani, biryani]   \n",
       "3                   [chili, honey, lotus, stem]   \n",
       "4                  [soup, honey, chilli, lotus]   \n",
       "\n",
       "                                      Cuisine_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, ...   \n",
       "\n",
       "                          Review_Preprocessed_No_Pos  \\\n",
       "0  [must, try, great, food, great, ambience, thnx...   \n",
       "1  [food, good, order, kodi, drumstick, basket, m...   \n",
       "2  [well, read, many, review, finally, visit, amb...   \n",
       "3  [come, birthday, treat, close, friend, perfect...   \n",
       "4  [food, good, soup, expect, starter, order, hon...   \n",
       "\n",
       "          Review_Preprocessed_No_Pos_Word2Vec_Vector  \\\n",
       "0  [0.17076543, 0.25489318, 0.13064787, -0.029729...   \n",
       "1  [0.040961727, 0.27078453, 0.20895234, 0.050825...   \n",
       "2  [0.05618261, 0.20712899, 0.18163988, -0.000625...   \n",
       "3  [0.12596513, 0.18433549, 0.12829378, -0.007487...   \n",
       "4  [0.055219177, 0.18505082, 0.16418888, 0.081227...   \n",
       "\n",
       "                               meals_Word2Vec_Vector  \\\n",
       "0  [-0.00069748174, 0.22289085, 0.1799066, 0.0503...   \n",
       "1  [-0.14319071, 0.1444223, 0.10494959, -0.111831...   \n",
       "2  [-0.11835837, 0.1308477, 0.1284879, -0.1177104...   \n",
       "3  [-0.11019059, 0.1285018, 0.11618678, -0.114508...   \n",
       "4  [-0.1173533, 0.10339392, 0.12608914, -0.057435...   \n",
       "\n",
       "                                  meals_Count_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "             Review_Preprocessed_No_Pos_Count_Vector  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                  meals_TFIDF_Vector  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "             Review_Preprocessed_No_Pos_TFIDF_Vector  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_per_row.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.73      0.30        82\n",
      "           1       0.12      0.70      0.21        64\n",
      "           2       0.04      0.58      0.08        24\n",
      "           3       0.31      0.82      0.45       176\n",
      "           4       0.08      0.84      0.15        25\n",
      "           5       0.31      0.82      0.45        89\n",
      "           6       0.15      0.74      0.24        74\n",
      "           7       0.38      0.76      0.50       161\n",
      "           8       0.16      0.75      0.26        56\n",
      "           9       0.24      0.76      0.37        95\n",
      "          10       0.58      0.72      0.64       479\n",
      "          11       0.34      0.71      0.46       243\n",
      "          12       0.47      0.72      0.57       186\n",
      "          13       0.09      0.78      0.16        37\n",
      "          14       0.30      0.72      0.42       147\n",
      "          15       0.08      0.72      0.15        32\n",
      "          16       0.03      0.88      0.05         8\n",
      "          17       0.04      0.67      0.08        15\n",
      "          18       0.13      0.74      0.23        47\n",
      "          19       0.23      0.96      0.37        24\n",
      "          20       0.04      0.50      0.07        14\n",
      "          21       0.30      0.81      0.43       163\n",
      "          22       0.05      0.77      0.10        22\n",
      "          23       0.04      0.67      0.08        15\n",
      "          24       0.11      0.70      0.19        66\n",
      "          25       0.12      0.81      0.21        16\n",
      "          26       0.14      0.85      0.24        47\n",
      "          27       0.03      0.89      0.07         9\n",
      "          28       0.08      0.74      0.15        23\n",
      "          29       0.19      0.82      0.31        60\n",
      "          30       0.16      0.70      0.27        79\n",
      "          31       0.05      0.79      0.10        14\n",
      "          32       0.76      0.79      0.78       653\n",
      "          33       0.10      0.67      0.17         6\n",
      "          34       0.08      0.69      0.15        49\n",
      "          35       0.09      0.78      0.16        41\n",
      "          36       0.13      0.57      0.21       106\n",
      "          37       0.05      0.81      0.09        16\n",
      "          38       0.08      0.89      0.14        19\n",
      "          39       0.10      0.85      0.19        47\n",
      "          40       0.08      0.63      0.14        30\n",
      "          41       0.03      0.64      0.05        11\n",
      "\n",
      "   micro avg       0.21      0.75      0.32      3570\n",
      "   macro avg       0.17      0.75      0.25      3570\n",
      "weighted avg       0.38      0.75      0.46      3570\n",
      " samples avg       0.23      0.77      0.33      3570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train_q1, X_test_q1, y_train_q1, y_test_q1 = train_test_split(data_per_row['Review_Preprocessed_No_Pos_Word2Vec_Vector'].tolist(), data_per_row['Cuisine_Vector'].tolist(), test_size=0.2, random_state=0)\n",
    "\n",
    "ovm = OneVsRestClassifier(LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\")).fit(X_train_q1, y_train_q1)\n",
    "y_pred_q1_ovm = ovm.predict(X_test_q1)\n",
    "\n",
    "print(classification_report(y_test_q1, y_pred_q1_ovm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Modelling without data leakage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "data_per_row_train, data_per_row_test = train_test_split(data_per_row, test_size=0.2, random_state=42, shuffle=True)\n",
    "data_grouped_train, data_grouped_test = train_test_split(data_grouped, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "def train_word2vec_and_generate_vectors(dataframe_train, dataframe_test, column_name, vector_size=100, window=5, min_count=1, workers=4, sg=1):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the specified column of the training dataframe and generates vectors for both train and test data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the Word2Vec vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    # Train the Word2Vec model on the training data\n",
    "    model = Word2Vec(\n",
    "        sentences=dataframe_train[column_name].tolist(),\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=sg\n",
    "    )\n",
    "\n",
    "    # Function to generate vectors for a given sentence\n",
    "    def get_vector(entry):\n",
    "        words = entry\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in model.wv:\n",
    "                vectors.append(model.wv[word])\n",
    "            else:\n",
    "                # 2. Zero Vector:\n",
    "                 vectors.append(np.zeros(vector_size))\n",
    "        if vectors:\n",
    "            return np.sum(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "\n",
    "    # Apply the function to both train and test dataframes\n",
    "    dataframe_train[f'{column_name}_Word2Vec_Vector'] = dataframe_train[column_name].apply(get_vector)\n",
    "    dataframe_test[f'{column_name}_Word2Vec_Vector'] = dataframe_test[column_name].apply(get_vector)\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "# Apply the function to the 'Review_Preprocessed_No_Pos' column\n",
    "data_per_row_train, data_per_row_test = train_word2vec_and_generate_vectors(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = train_word2vec_and_generate_vectors(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = train_word2vec_and_generate_vectors(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = train_word2vec_and_generate_vectors(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def create_count_vectorizer_vector(dataframe_train, dataframe_test, column):\n",
    "    \"\"\"\n",
    "    Creates a CountVectorizer vector for each row in the specified column of the training and testing dataframes.\n",
    "    \n",
    "    Args:\n",
    "        dataframe_train (pd.DataFrame): The training DataFrame containing the data.\n",
    "        dataframe_test (pd.DataFrame): The testing DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame, pd.DataFrame: The training and testing DataFrames with new columns containing the CountVectorizer vectors.\n",
    "    \"\"\"\n",
    "    # Create a CountVectorizer object\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column of the training dataframe\n",
    "    vectorizer.fit([item for sublist in dataframe_train[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually for the training dataframe\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column for the training dataframe\n",
    "    dataframe_train[f'{column}_Count_Vector'] = dataframe_train[column].apply(transform_row)\n",
    "    # Apply the function to each row and create a new column for the testing dataframe\n",
    "    dataframe_test[f'{column}_Count_Vector'] = dataframe_test[column].apply(transform_row)\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "data_per_row_train, data_per_row_test = create_count_vectorizer_vector(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = create_count_vectorizer_vector(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = create_count_vectorizer_vector(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = create_count_vectorizer_vector(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tfidf_vectorize(dataframe_train, dataframe_test, column):\n",
    "    \"\"\"\n",
    "    Creates a TF-IDF vector for each row in the specified column of the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing the data.\n",
    "        column (str): The name of the column to be vectorized.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column containing the TF-IDF vectors.\n",
    "    \"\"\"\n",
    "    # Create a TF-IDF vectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer on the entire column\n",
    "    vectorizer.fit([item for sublist in dataframe_train[column] for item in sublist])\n",
    "\n",
    "    # Transform each row individually\n",
    "    def transform_row(row):\n",
    "        row_str = \" \".join(row)  # Convert list to string\n",
    "        X = vectorizer.transform([row_str])\n",
    "        return X.toarray()[0]\n",
    "\n",
    "    # Apply the function to each row and create a new column\n",
    "    dataframe_train[f'{column}_TFIDF_Vector'] = dataframe_train[column].apply(transform_row)\n",
    "    dataframe_test[f'{column}_TFIDF_Vector'] = dataframe_test[column].apply(transform_row)\n",
    "\n",
    "\n",
    "    return dataframe_train, dataframe_test\n",
    "\n",
    "data_per_row_train, data_per_row_test = tfidf_vectorize(data_per_row_train, data_per_row_test, 'Review_Preprocessed_No_Pos')\n",
    "data_per_row_train, data_per_row_test = tfidf_vectorize(data_per_row_train, data_per_row_test, 'meals')\n",
    "\n",
    "data_grouped_train, data_grouped_test = tfidf_vectorize(data_grouped_train, data_grouped_test, 'Review_Preprocessed_No_Pos')\n",
    "data_grouped_train, data_grouped_test = tfidf_vectorize(data_grouped_train, data_grouped_test, 'meals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/108 [00:21<37:32, 21.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with ClassifierChain  LogisticRegression  meals_Word2Vec_Vector on grouped: \n",
      "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|         | 5/108 [01:08<24:23, 14.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with MultiOutput  LogisticRegression  meals_Word2Vec_Vector on grouped: \n",
      "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|         | 6/108 [02:03<35:04, 20.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[189], line 89\u001b[0m\n\u001b[0;32m     77\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeals_Word2Vec_Vector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview_Preprocessed_No_Pos_Word2Vec_Vector\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview_Preprocessed_No_Pos_TFIDF_Vector\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     86\u001b[0m ]\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Usage\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_evaluate_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train_per_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_per_row_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_test_per_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_per_row_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_train_grouped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_grouped_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf_test_grouped\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_grouped_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCuisine_Vector\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     96\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Performance Summary:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m results\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_f1\u001b[39m\u001b[38;5;124m'\u001b[39m], ascending\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28;01mFalse\u001b[39;00m])\n",
      "Cell \u001b[1;32mIn[189], line 60\u001b[0m, in \u001b[0;36mtrain_evaluate_models\u001b[1;34m(df_train_per_row, df_test_per_row, df_train_grouped, df_test_grouped, feature_cols, target_col)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Create and train model\u001b[39;00m\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m wrapper(base_model)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Predict and evaluate\u001b[39;00m\n\u001b[0;32m     63\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\multioutput.py:813\u001b[0m, in \u001b[0;36mClassifierChain.fit\u001b[1;34m(self, X, Y)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[0;32m    797\u001b[0m \n\u001b[0;32m    798\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;124;03m    Class instance.\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 813\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    815\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m chain_idx, estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_)\n\u001b[0;32m    816\u001b[0m ]\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\multioutput.py:632\u001b[0m, in \u001b[0;36m_BaseChain.fit\u001b[1;34m(self, X, Y, **fit_params)\u001b[0m\n\u001b[0;32m    630\u001b[0m y \u001b[38;5;241m=\u001b[39m Y[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morder_[chain_idx]]\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChain\u001b[39m\u001b[38;5;124m\"\u001b[39m, message):\n\u001b[1;32m--> 632\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X_aug[:, : (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m chain_idx)], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m chain_idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    634\u001b[0m     col_idx \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m chain_idx\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\parallel.py:1051\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1049\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1051\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\parallel.py:864\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 864\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\parallel.py:782\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    781\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 782\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\joblib\\parallel.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    264\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\tree\\_classes.py:889\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    859\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    861\u001b[0m \n\u001b[0;32m    862\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 889\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wojci\\anaconda3\\envs\\text-mining1\\lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    return {\n",
    "        'model': model_name,\n",
    "        'weighted_f1': report['weighted avg']['f1-score'],\n",
    "        'macro_f1': report['macro avg']['f1-score']\n",
    "    }\n",
    "\n",
    "def train_evaluate_models(df_train_per_row, df_test_per_row, \n",
    "                         df_train_grouped, df_test_grouped, \n",
    "                         feature_cols, target_col):\n",
    "    \"\"\"Train and evaluate models with different data structures\"\"\"\n",
    "    \n",
    "    # Define base models\n",
    "    base_models = {\n",
    "        'LogisticRegression': LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\", max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "        'LinearSVC': LinearSVC(random_state=0, max_iter=2000)\n",
    "    }\n",
    "    \n",
    "    # Define wrapper models\n",
    "    wrappers = {\n",
    "        'ClassifierChain': ClassifierChain,\n",
    "        'OneVsRest': OneVsRestClassifier,\n",
    "        'MultiOutput': MultiOutputClassifier\n",
    "    }\n",
    "    \n",
    "    # Define DataFrame types\n",
    "    df_types = {\n",
    "        'per_row': (df_train_per_row, df_test_per_row),\n",
    "        'grouped': (df_train_grouped, df_test_grouped)\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Iterate through all combinations\n",
    "    combinations = list(product(feature_cols, base_models.items(), wrappers.items(), df_types.items()))\n",
    "    for feature_col, (base_name, base_model), (wrapper_name, wrapper), (df_type, (train_df, test_df)) in tqdm(combinations):\n",
    "        try:\n",
    "            # Prepare data\n",
    "            X_train = np.vstack(train_df[feature_col].values)\n",
    "            X_test = np.vstack(test_df[feature_col].values)\n",
    "            y_train = np.array(train_df[target_col].tolist())\n",
    "            y_test = np.array(test_df[target_col].tolist())\n",
    "            \n",
    "            # Create and train model\n",
    "            model = wrapper(base_model)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Predict and evaluate\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Store results\n",
    "            eval_result = evaluate_model(y_test, y_pred, f\"{wrapper_name}_{base_name}_{feature_col}\")\n",
    "            eval_result['feature_col'] = feature_col\n",
    "            eval_result['df_type'] = df_type\n",
    "            results.append(eval_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {wrapper_name}  {base_name}  {feature_col} on {df_type}: \\n{str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "feature_columns = [\n",
    "    'meals_Word2Vec_Vector',\n",
    "    'Review_Preprocessed_No_Pos_Word2Vec_Vector',\n",
    "\n",
    "    'meals_Count_Vector',    \n",
    "    'Review_Preprocessed_No_Pos_Count_Vector',\n",
    "\n",
    "    'meals_TFIDF_Vector',    \n",
    "    'Review_Preprocessed_No_Pos_TFIDF_Vector'\n",
    "]\n",
    "\n",
    "# Usage\n",
    "results = train_evaluate_models(\n",
    "    df_train_per_row=data_per_row_train,\n",
    "    df_test_per_row=data_per_row_test,\n",
    "    df_train_grouped=data_grouped_train,\n",
    "    df_test_grouped=data_grouped_test,\n",
    "    feature_cols=feature_columns,\n",
    "    target_col='Cuisine_Vector'\n",
    ")\n",
    "\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "results.sort_values(['weighted_f1'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 108/108 [32:42<00:00, 18.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import ClassifierChain, MultiOutputClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def evaluate_single_model(feature_col, base_model, wrapper, df_type, train_df, test_df, target_col):\n",
    "    try:\n",
    "        # Prepare data\n",
    "        X_train = np.vstack(train_df[feature_col].values)\n",
    "        X_test = np.vstack(test_df[feature_col].values)\n",
    "        y_train = np.array(train_df[target_col].tolist())\n",
    "        y_test = np.array(test_df[target_col].tolist())\n",
    "        \n",
    "        # Quick check for class distribution\n",
    "        if len(np.unique(y_train)) < 2:\n",
    "            return None\n",
    "        \n",
    "        # Create and train model\n",
    "        model = wrapper(base_model)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluate_model(y_test, y_pred, f\"{wrapper.__name__}_{base_model.__class__.__name__}_{feature_col}\")\n",
    "        eval_result['feature_col'] = feature_col\n",
    "        eval_result['df_type'] = df_type\n",
    "        return eval_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {wrapper.__name__}_{base_model.__class__.__name__}_{feature_col} on {df_type}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def train_evaluate_models(df_train_per_row, df_test_per_row, \n",
    "                         df_train_grouped, df_test_grouped, \n",
    "                         feature_cols, target_col, n_jobs=-1):\n",
    "    \"\"\"Parallel training and evaluation of models\"\"\"\n",
    "    \n",
    "    # Define base models - initialize once\n",
    "    base_models = {\n",
    "        'LogisticRegression': LogisticRegression(solver='lbfgs', random_state=0, class_weight=\"balanced\", max_iter=1000),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "        'LinearSVC': LinearSVC(random_state=0, max_iter=2000)\n",
    "    }\n",
    "    \n",
    "    # Define wrapper models\n",
    "    wrappers = {\n",
    "        'ClassifierChain': ClassifierChain,\n",
    "        'OneVsRest': OneVsRestClassifier,\n",
    "        'MultiOutput': MultiOutputClassifier\n",
    "    }\n",
    "    \n",
    "    # Define DataFrame types\n",
    "    df_types = {\n",
    "        'per_row': (df_train_per_row, df_test_per_row),\n",
    "        'grouped': (df_train_grouped, df_test_grouped)\n",
    "    }\n",
    "    \n",
    "    # Create all combinations\n",
    "    combinations = [\n",
    "        (feature_col, base_model, wrapper, df_type, train_df, test_df)\n",
    "        for feature_col in feature_cols\n",
    "        for base_model in base_models.values()\n",
    "        for wrapper in wrappers.values()\n",
    "        for df_type, (train_df, test_df) in df_types.items()\n",
    "    ]\n",
    "    \n",
    "    # Run parallel evaluation\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(evaluate_single_model)(\n",
    "            feature_col, base_model, wrapper, df_type, train_df, test_df, target_col\n",
    "        )\n",
    "        for feature_col, base_model, wrapper, df_type, train_df, test_df in tqdm(combinations)\n",
    "    )\n",
    "    \n",
    "    # Filter None results and convert to DataFrame\n",
    "    results = [r for r in results if r is not None]\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage remains the same\n",
    "results = train_evaluate_models(\n",
    "    df_train_per_row=data_per_row_train,\n",
    "    df_test_per_row=data_per_row_test,\n",
    "    df_train_grouped=data_grouped_train,\n",
    "    df_test_grouped=data_grouped_test,\n",
    "    feature_cols=feature_columns,\n",
    "    target_col='Cuisine_Vector'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results.to_csv('data_hyderabad/all_model_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>feature_col</th>\n",
       "      <th>df_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>MultiOutputClassifier_LogisticRegression_Revie...</td>\n",
       "      <td>0.610538</td>\n",
       "      <td>0.492882</td>\n",
       "      <td>Review_Preprocessed_No_Pos_TFIDF_Vector</td>\n",
       "      <td>per_row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>OneVsRestClassifier_LogisticRegression_Review_...</td>\n",
       "      <td>0.610538</td>\n",
       "      <td>0.492882</td>\n",
       "      <td>Review_Preprocessed_No_Pos_TFIDF_Vector</td>\n",
       "      <td>per_row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>OneVsRestClassifier_LogisticRegression_Review_...</td>\n",
       "      <td>0.608301</td>\n",
       "      <td>0.477934</td>\n",
       "      <td>Review_Preprocessed_No_Pos_Count_Vector</td>\n",
       "      <td>per_row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>MultiOutputClassifier_LogisticRegression_Revie...</td>\n",
       "      <td>0.608301</td>\n",
       "      <td>0.477934</td>\n",
       "      <td>Review_Preprocessed_No_Pos_Count_Vector</td>\n",
       "      <td>per_row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>ClassifierChain_LinearSVC_Review_Preprocessed_...</td>\n",
       "      <td>0.605337</td>\n",
       "      <td>0.476202</td>\n",
       "      <td>Review_Preprocessed_No_Pos_TFIDF_Vector</td>\n",
       "      <td>per_row</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>OneVsRestClassifier_LogisticRegression_Review_...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>Review_Preprocessed_No_Pos_Word2Vec_Vector</td>\n",
       "      <td>grouped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ClassifierChain_RandomForestClassifier_Review_...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>Review_Preprocessed_No_Pos_Word2Vec_Vector</td>\n",
       "      <td>grouped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OneVsRestClassifier_RandomForestClassifier_mea...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>meals_Word2Vec_Vector</td>\n",
       "      <td>grouped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>OneVsRestClassifier_RandomForestClassifier_Rev...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>Review_Preprocessed_No_Pos_Word2Vec_Vector</td>\n",
       "      <td>grouped</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ClassifierChain_RandomForestClassifier_meals_W...</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>meals_Word2Vec_Vector</td>\n",
       "      <td>grouped</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  weighted_f1  macro_f1  \\\n",
       "73  MultiOutputClassifier_LogisticRegression_Revie...     0.610538  0.492882   \n",
       "71  OneVsRestClassifier_LogisticRegression_Review_...     0.610538  0.492882   \n",
       "43  OneVsRestClassifier_LogisticRegression_Review_...     0.608301  0.477934   \n",
       "45  MultiOutputClassifier_LogisticRegression_Revie...     0.608301  0.477934   \n",
       "80  ClassifierChain_LinearSVC_Review_Preprocessed_...     0.605337  0.476202   \n",
       "..                                                ...          ...       ...   \n",
       "16  OneVsRestClassifier_LogisticRegression_Review_...     0.101010  0.015873   \n",
       "19  ClassifierChain_RandomForestClassifier_Review_...     0.101010  0.015873   \n",
       "7   OneVsRestClassifier_RandomForestClassifier_mea...     0.101010  0.015873   \n",
       "21  OneVsRestClassifier_RandomForestClassifier_Rev...     0.101010  0.015873   \n",
       "5   ClassifierChain_RandomForestClassifier_meals_W...     0.101010  0.015873   \n",
       "\n",
       "                                   feature_col  df_type  \n",
       "73     Review_Preprocessed_No_Pos_TFIDF_Vector  per_row  \n",
       "71     Review_Preprocessed_No_Pos_TFIDF_Vector  per_row  \n",
       "43     Review_Preprocessed_No_Pos_Count_Vector  per_row  \n",
       "45     Review_Preprocessed_No_Pos_Count_Vector  per_row  \n",
       "80     Review_Preprocessed_No_Pos_TFIDF_Vector  per_row  \n",
       "..                                         ...      ...  \n",
       "16  Review_Preprocessed_No_Pos_Word2Vec_Vector  grouped  \n",
       "19  Review_Preprocessed_No_Pos_Word2Vec_Vector  grouped  \n",
       "7                        meals_Word2Vec_Vector  grouped  \n",
       "21  Review_Preprocessed_No_Pos_Word2Vec_Vector  grouped  \n",
       "5                        meals_Word2Vec_Vector  grouped  \n",
       "\n",
       "[84 rows x 5 columns]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(['macro_f1'], ascending=[False])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text-mining1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
